{"version":3,"file":"index.395a21c8.js","sources":["../../src/components/ButtonOptions.tsx","../../src/components/ButtonHelp.tsx","../../src/store.ts","../../src/components/ButtonClearCache.tsx","../../src/lib/metaframe.ts","../../src/lib/TrainingData.ts","../../src/lib/Trainer.ts","../../src/lib/io.ts","../../src/routes/tensorflow/TabMetaframeTraining.tsx","../../src/routes/tensorflow/TabMetaframePrediction.tsx","../../src/components/Messages.tsx","../../src/routes/tensorflow/DataExamplePlot.tsx","../../src/routes/tensorflow/TabVisualizeTrainingData.tsx","../../src/routes/tensorflow/index.tsx","../../src/routes/home.tsx","../../src/App.tsx","../../src/index.tsx"],"sourcesContent":["import { h, FunctionalComponent } from \"preact\";\nimport { Option, OptionsMenuButton } from \"@metapages/metaframe-ui-widgets\";\n\nconst appOptions: Option[] = [\n  {\n    name: \"nocache\",\n    displayName: \"Disable caching\",\n    type: \"boolean\",\n    default: false,\n  },\n  {\n    name: \"hide_auto_help\",\n    displayName: \"Hide automatic help\",\n    type: \"boolean\",\n    default: false,\n  },\n];\n\n// wish I could extract this from Option[]\nexport type OptionBlob = {\n  nocache?: boolean;\n  hide_auto_help?: boolean;\n};\n\nexport const ButtonOptions: FunctionalComponent = () => {\n  return <OptionsMenuButton options={appOptions} />;\n};\n","import { h, Fragment, FunctionalComponent } from \"preact\";\nimport { useCallback, useState } from \"preact/hooks\";\nimport {\n  Drawer,\n  DrawerOverlay,\n  DrawerContent,\n  DrawerBody,\n  IconButton,\n  DrawerCloseButton,\n} from \"@chakra-ui/react\";\nimport { isIframe, useHashParamJson } from \"@metapages/metaframe-hook\";\nimport { QuestionIcon } from \"@chakra-ui/icons\";\nimport { OptionBlob } from \"./ButtonOptions\";\n\nexport const ButtonHelp: FunctionalComponent<{ url?: string }> = ({ url }) => {\n  // from OptionsMenuButton\n  const [optionsInHashParams] = useHashParamJson<OptionBlob>(\"options\");\n  const hideAutoHelp: boolean = (optionsInHashParams &&\n    optionsInHashParams.hide_auto_help)!!;\n\n  const [open, setOpen] = useState<boolean>(isIframe() ? false : !hideAutoHelp);\n\n  const onClick = useCallback(() => {\n    setOpen(!open);\n  }, [open]);\n\n  url = url\n    ? url\n    : `${window.location.origin}${window.location.pathname}README.md`;\n\n  return (\n    <Fragment>\n      <IconButton\n        verticalAlign=\"top\"\n        aria-label=\"Help\"\n        // @ts-ignore\n        icon={<QuestionIcon />}\n        size=\"lg\"\n        onClick={onClick}\n        mr=\"4\"\n      />\n      <HelpPanel url={url} isOpen={open} setOpen={setOpen} />\n    </Fragment>\n  );\n};\n\nconst HelpPanel: FunctionalComponent<{\n  url: string;\n  isOpen: boolean;\n  setOpen: (open: boolean) => void;\n}> = ({ isOpen, setOpen, url }) => {\n  const onClose = useCallback(() => {\n    setOpen(!isOpen);\n  }, [setOpen, isOpen]);\n\n  const onOverlayClick = useCallback(() => {\n    setOpen(false);\n  }, [setOpen]);\n\n  const iframeUrl = `https://metapages.github.io/metaframe-markdown/#?url=${url}`;\n\n  return (\n    <Drawer\n      size=\"full\"\n      placement=\"top\"\n      onClose={onClose}\n      isOpen={isOpen}\n      onOverlayClick={onOverlayClick}\n    >\n      <DrawerOverlay>\n        <DrawerContent>\n          <DrawerCloseButton size=\"lg\" colorScheme=\"blue.500\" bg=\"gray.100\" />\n          <DrawerBody>\n            <iframe width=\"100%\" height=\"100%\" src={iframeUrl} />\n          </DrawerBody>\n        </DrawerContent>\n      </DrawerOverlay>\n    </Drawer>\n  );\n};\n","import create from \"zustand\";\nimport * as tf from \"@tensorflow/tfjs\";\nimport {\n  AlertStatus,\n} from \"@chakra-ui/react\";\nimport { PredictionResult, TrainingDataSet, PredictionInputEncoded } from './lib/metaframe';\nimport { PersistedModel } from \"./lib/types\";\n\nexport type MessagePayload = {\n  message: string;\n  type: AlertStatus;\n  messages?: string[];\n};\n\n\nexport type StoreState = {\n  messages: MessagePayload[];\n  modelCount: number;\n  model: PersistedModel | undefined;\n  trainingDataSet: TrainingDataSet | undefined;\n  predictionInput: PredictionInputEncoded | undefined;\n  predictionOutput: PredictionResult | undefined;\n  currentlyTrainingDataHash: string | null;\n  clearMessages: () => void;\n  addMessage: (message: MessagePayload) => void;\n  setMessages: (messages: MessagePayload[]) => void;\n  setTrainingDataSet: (training: TrainingDataSet) => void;\n  updateModels: () => Promise<void>;\n  deleteModels: () => void;\n  setModel: (model: PersistedModel) => void;\n  setPredictionInput: (prediction: PredictionInputEncoded) => void;\n  setPredictionOutput: (prediction: PredictionResult) => void;\n};\n\nexport const useStore = create<StoreState>(set => ({\n  messages: [],\n  modelCount: -1,\n  model: undefined,\n  trainingDataSet: undefined,\n  predictionInput: undefined,\n  predictionOutput: undefined,\n  currentlyTrainingDataHash: null,\n  clearMessages: () => set((state) => ({ messages: [] })),\n  addMessage: (message: MessagePayload) =>\n    set((state) => ({ messages: state.messages.concat([message]) })),\n  setMessages: (messages: MessagePayload[]) => set((state) => ({ messages })),\n  setTrainingDataSet: (trainingDataSet: TrainingDataSet) =>\n    set((state) => ({ trainingDataSet })),\n\n  updateModels: async () => {\n    const count = await getModelCount();\n    set((state) => ({ modelCount: count }));\n  },\n  deleteModels: async () => {\n    await deleteAllModels();\n    set((state) => ({ modelCount: 0 }));\n  },\n  setModel: async (model: PersistedModel) => {\n    set((state) => ({ model }));\n  },\n  setPredictionInput: async (predictionInput: PredictionInputEncoded) => {\n    set((state) => ({ predictionInput }));\n  },\n  setPredictionOutput: async (predictionOutput: PredictionResult) => {\n    set((state) => ({ predictionOutput }));\n  },\n}));\n\nconst getModelCount = async () => {\n  const allModels = await tf.io.listModels();\n  return Object.keys(allModels).length;\n};\n\nconst deleteAllModels = async () => {\n  const allModels = await tf.io.listModels();\n  const deletions = Object.keys(allModels).map((key) => tf.io.removeModel(key));\n  try {\n    await Promise.all(deletions);\n  } catch (err) {\n    console.error(err);\n  }\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport { useCallback, useEffect } from \"preact/hooks\";\nimport { Button } from \"@chakra-ui/react\";\nimport { useStore } from \"../store\";\n\nexport const ButtonClearCache: FunctionalComponent = () => {\n  const modelCount = useStore((state) => state.modelCount);\n  const updateModels = useStore((state) => state.updateModels);\n  const deleteAllModels = useStore((state) => state.deleteModels);\n\n  const onClick = useCallback(async () => {\n    deleteAllModels();\n  }, [deleteAllModels]);\n\n  // update the model count on load\n  useEffect(() => {\n    (async () => {\n      await updateModels();\n    })();\n  }, [updateModels]);\n\n  return (\n    <Button isDisabled={modelCount <= 0} onClick={onClick}>\n      Clear cache ({modelCount > 0 ? modelCount : 0} models)\n    </Button>\n  );\n};\n","export type Base64String = string;\n\n\n/**\n * Prediction/training types\n */\n\n/**\n * Prediction/training types\n */\n\n/* Generic map of keys to number arrays */\nexport interface SensorSeries {[key:string] : Float32Array};\nexport interface SensorSeriesBase64 {[key:string] : string };\n/* Main persisted form of a single complete sensor example (e.g. a gesture) */\nexport interface PredictionInputEncoded {\n  series: SensorSeriesBase64;\n  requestId ?: string | number;\n}\n\nexport interface PredictionInput {\n  series: SensorSeries;\n  requestId ?: string | number;\n}\n\nexport const sensorSeriesDecode:(series :SensorSeriesBase64) => SensorSeries = series => {\n  const result :SensorSeries = {};\n  Object.keys(series).forEach(k => {\n    if (series[k]) {\n      result[k] = new Float32Array(base64decode(series[k]));\n    }\n  });\n  return result;\n}\n\nexport const predictionDecode:(prediction :PredictionInputEncoded) => PredictionInput = prediction => {\n  console.log('predictionDecode', prediction);\n  const result :PredictionInput = {\n    requestId: prediction.requestId,\n    series: sensorSeriesDecode(prediction.series),\n  };\n  return result;\n}\n\nexport const predictionEncode:(prediction :PredictionInput) => PredictionInputEncoded = prediction => {\n  const result :PredictionInputEncoded = {\n    requestId: prediction.requestId,\n    series: sensorSeriesEncode(prediction.series),\n  };\n  return result;\n}\n\nexport const sensorSeriesEncode:(series :SensorSeries) => SensorSeriesBase64 = series => {\n  const result :SensorSeriesBase64 = {};\n  Object.keys(series).forEach(k => result[k] = base64encode(series[k].buffer));\n  return result;\n}\n\n// export const convertIMUSensorJsonToExample :(example:IMUSensorJson) => IMUSensorExample = example => {\n//   console.log('convertIMUSensorJsonToExample example', example);\n//   return {\n//       ax: new Float32Array(base64decode(example.ax)),\n//       ay: new Float32Array(base64decode(example.ay)),\n//       az: new Float32Array(base64decode(example.az)),\n//       at: new Int32Array(base64decode(example.at)),\n//       gx: new Float32Array(base64decode(example.gx)),\n//       gy: new Float32Array(base64decode(example.gy)),\n//       gz: new Float32Array(base64decode(example.gz)),\n//       gt: new Int32Array(base64decode(example.gt)),\n//   }\n// }\n\n\n\n\n\n/**\n * This is the output from an input prection\n * prediction: the class name of the prediction\n * predictions: map of class names to score\n */\nexport interface PredictionResult {\n  prediction: string;\n  predictions: {\n    [className: string]: number\n  };\n  requestId ?: string | number;\n  modelHash: string;\n  modelId?: string;\n  note?:string;\n}\n\nexport interface TrainingDataPoint {\n  version?: string; // TODO: solve versioning since this is a blob to the graphql API\n  name?: string;\n  label: string;\n  url?: string;\n  data: PredictionInputEncoded; // JSON transfer uses Base64String\n}\n\nexport interface TrainingDataSet {\n  // stored with the model, for bookkeeping\n  modelId?: string;\n  examples: Array<TrainingDataPoint>;\n  /* examples for \"no real gesture\" */\n  controlLabels?:string[];\n  hash:string; // computed using object-hash\n}\n\nexport interface PredictionMetadata {\n  classNames: string[];\n  imageHeight: number;\n  imageWidth: number;\n\n  // maxAbsoluteRawValue: number;\n}\n\nexport interface TrainingMetadata {\n  date: Date;\n  hash: string;\n  // Derived from training data, use for normalizing predictions\n  // keys are series labels, e.g. ax, gy etc (**not** example labels)\n  ranges: Record<string, {min:number, max:number, absmax:number}>;\n}\n\n\n/**\n * Low specific types\n */\n\n\n/**\n * Handle streaming IMU data and convert to more friendly formats.\n * The internal data format is a dict of Float32Arrays representing\n * 1D time series from sensors. This should be easy to extend to arbitrary\n * sensor streams (or any set of 1D series).\n */\n\n\nexport interface IMUPoint {\n  x: number;\n  y: number;\n  z: number;\n  t: number;\n}\n\n// this could be any combination of points depending on when they're streaming in\nexport interface IMUPointCombined {\n  ax?: number;\n  ay?: number;\n  az?: number;\n  at?: number;\n  gx?: number;\n  gy?: number;\n  gz?: number;\n  gt?: number;\n  t?: number;\n}\n\n\nexport interface IMUPointCombinedExample {\n  data :Array<IMUPointCombined>;\n}\nexport interface IMUSensorGesture {\n  accelerometer :Array<IMUPoint>;\n  gyroscope :Array<IMUPoint>;\n}\n\n\n\nexport interface IMUGestureChunk {\n  a ?: IMUPoint;\n  g ?: IMUPoint;\n  event? :string;\n}\n\nexport interface IMUSensorJson {\n  ax :Base64String;\n  ay :Base64String;\n  az :Base64String;\n  at :Base64String;\n  gx :Base64String;\n  gy :Base64String;\n  gz :Base64String;\n  gt :Base64String;\n}\n\nexport interface IMUSensorExample {\n  ax :Float32Array;\n  ay :Float32Array;\n  az :Float32Array;\n  at :Int32Array;\n  gx :Float32Array;\n  gy :Float32Array;\n  gz :Float32Array;\n  gt :Int32Array;\n}\n\n\n// export interface PredictionInput {\n//     data: {[key:string] : string | Float32Array | Int32Array};\n//     requestId: string | number;\n//   }\n\n\n// the sensor gesture needs to be compacted\n// export const convertIMUSensorGestureToIMUSensorExample :(gesture:IMUPointCombinedExample) => IMUSensorExample = gesture => {\n\n//   return {\n//       ax: new Float32Array(gesture.accelerometer.map(getX)),\n//       ay: new Float32Array(gesture.accelerometer.map(getY)),\n//       az: new Float32Array(gesture.accelerometer.map(getZ)),\n//       at: new Float32Array(gesture.accelerometer.map(getT)),\n//       gx: new Float32Array(gesture.gyroscope.map(getX)),\n//       gy: new Float32Array(gesture.gyroscope.map(getY)),\n//       gz: new Float32Array(gesture.gyroscope.map(getZ)),\n//       gt: new Float32Array(gesture.gyroscope.map(getT)),\n//   }\n// }\n\n\n\n\n// the sensor gesture needs to be compacted\nexport const convertIMUSensorExampleToJson :(example:IMUSensorExample) => IMUSensorJson = example => {\n  return {\n      ax: base64encode(example.ax.buffer),\n      ay: base64encode(example.ay.buffer),\n      az: base64encode(example.az.buffer),\n      at: base64encode(example.at.buffer),\n      gx: base64encode(example.gx.buffer),\n      gy: base64encode(example.gy.buffer),\n      gz: base64encode(example.gz.buffer),\n      gt: base64encode(example.gt.buffer),\n  }\n}\n\n// SensorSeries\n\n// the sensor gesture needs to be compacted\nexport const convertIMUSensorJsonToExample :(example:IMUSensorJson) => IMUSensorExample = example => {\n  return {\n      ax: new Float32Array(base64decode(example.ax)),\n      ay: new Float32Array(base64decode(example.ay)),\n      az: new Float32Array(base64decode(example.az)),\n      at: new Int32Array(base64decode(example.at)),\n      gx: new Float32Array(base64decode(example.gx)),\n      gy: new Float32Array(base64decode(example.gy)),\n      gz: new Float32Array(base64decode(example.gz)),\n      gt: new Int32Array(base64decode(example.gt)),\n  }\n}\n\n\n\n/*\n * base64-arraybuffer\n * https://github.com/niklasvh/base64-arraybuffer\n *\n * Copyright (c) 2012 Niklas von Hertzen\n * Licensed under the MIT license.\n */\n\nconst chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\";\n\n// Use a lookup table to find the index.\nconst lookup = new Uint8Array(256);\nfor (var i = 0; i < chars.length; i++) {\n  lookup[chars.charCodeAt(i)] = i;\n}\n\nexport function base64encode(arraybuffer : ArrayBuffer) :string {\n  let bytes = new Uint8Array(arraybuffer);\n  let i: number;\n  let len = bytes.length;\n  let base64 = \"\";\n\n  for (i = 0; i < len; i += 3) {\n    base64 += chars[bytes[i] >> 2];\n    base64 += chars[((bytes[i] & 3) << 4) | (bytes[i + 1] >> 4)];\n    base64 += chars[((bytes[i + 1] & 15) << 2) | (bytes[i + 2] >> 6)];\n    base64 += chars[bytes[i + 2] & 63];\n  }\n\n  if (len % 3 === 2) {\n    base64 = base64.substring(0, base64.length - 1) + \"=\";\n  } else if (len % 3 === 1) {\n    base64 = base64.substring(0, base64.length - 2) + \"==\";\n  }\n\n  return base64;\n}\n\nexport function base64decode(base64 : string) :ArrayBuffer {\n    if (!base64) {\n        throw new Error(\"base64decode string argument given\");\n    }\n  let bufferLength = base64.length * 0.75,\n    len = base64.length,\n    i:number,\n    p = 0,\n    encoded1: number,\n    encoded2: number,\n    encoded3: number,\n    encoded4: number;\n\n  if (base64[base64.length - 1] === \"=\") {\n    bufferLength--;\n    if (base64[base64.length - 2] === \"=\") {\n      bufferLength--;\n    }\n  }\n\n  var arraybuffer = new ArrayBuffer(bufferLength),\n    bytes = new Uint8Array(arraybuffer);\n\n  for (i = 0; i < len; i += 4) {\n    encoded1 = lookup[base64.charCodeAt(i)];\n    encoded2 = lookup[base64.charCodeAt(i + 1)];\n    encoded3 = lookup[base64.charCodeAt(i + 2)];\n    encoded4 = lookup[base64.charCodeAt(i + 3)];\n\n    bytes[p++] = (encoded1 << 2) | (encoded2 >> 4);\n    bytes[p++] = ((encoded2 & 15) << 4) | (encoded3 >> 2);\n    bytes[p++] = ((encoded3 & 3) << 6) | (encoded4 & 63);\n  }\n\n  return arraybuffer;\n}\n","/**\n * @license\n * Private wand license.\n * Currently very specific to gestures, need to loosen that up\n */\n\n/**\n * Some terminology\n * length or height of data set: in this case the time steps in the gyrometer samples.\n * width or depth: 6 (2 * [x,y,z] for accelerometer and gyrometer resp.)\n */\n\n\n\n// const IMAGE_SIZE = 784;\n// const NUM_CLASSES = 10;\n// const NUM_DATASET_ELEMENTS = 65000;\n\n// const TRAIN_TEST_RATIO = 5 / 6;\n\n// const NUM_TRAIN_ELEMENTS = Math.floor(TRAIN_TEST_RATIO * NUM_DATASET_ELEMENTS);\n// const NUM_TEST_ELEMENTS = NUM_DATASET_ELEMENTS - NUM_TRAIN_ELEMENTS;\n\n// const MNIST_IMAGES_SPRITE_PATH =\n//     'https://storage.googleapis.com/learnjs-data/model-builder/mnist_images.png';\n// const MNIST_LABELS_PATH =\n//     'https://storage.googleapis.com/learnjs-data/model-builder/mnist_labels_uint8';\n\n/**\n * A class that fetches the sprited MNIST dataset and returns shuffled batches.\n *\n * NOTE: This will get much easier. For now, we do data fetching and\n * manipulation manually.\n */\nimport objectHash from \"object-hash\";\n// import { TrainingDataSet } from './types';\nimport * as tf from '@tensorflow/tfjs';\nimport { TrainingDataSet, TrainingDataPoint, SensorSeries, sensorSeriesDecode } from './metaframe';\n\n// const sensorNames = ['accelerometer', 'gyroscope'];\n// const axes = ['x', 'y', 'z'];\n// Proportion of all examples used for training. The rest are used for testing.\nconst proportionTrainingSamples = 0.7;\n\n// const count = (a, b) => a + b;\n\nexport type SensorJson = {data:SensorSeries, url:string};\n\n/**\n * async load() is where the important data filtering happens\n */\nexport class TrainingData {\n    trainingDataJson:TrainingDataSet;\n\n    // label: {data:exampleData, url:title}\n    // <GestureName, Array<SensorJson>>\n    data:{ [key: string]: SensorJson[]; } = {}; // Final processed data\n\n    // for normalizing predictions\n    ranges: Record<string, {min:number, max:number, absmax:number}> = {};\n\n    //\n    _labels:string[] = [];\n    // e.g. [ ax ay az gx gy gz ] computed from examples\n    _streams :string[] = [];\n\n    // From the previous thing, not yet able to use these, will do soon\n    shuffledTrainIndex:number = 0;\n    shuffledTestIndex:number = 0;\n\n    trainingIndices:Uint32Array = new Uint32Array();\n    testingIndices:Uint32Array = new Uint32Array();\n\n    bigFatDataArray:Float32Array = new Float32Array();\n    bigFatLabelArray:Uint8Array = new Uint8Array();\n\n    _timesteps:number = 0;\n    // _maxAbsoluteValue:number = 0;\n\n\n  constructor(trainingJson :TrainingDataSet) {\n    // The device data blob, holds URLs to the actual data\n    this.trainingDataJson = trainingJson;\n  }\n\n  hash() :string {\n    if (!this.trainingDataJson) {\n      throw \"No trainingDataJson to hash\";\n    }\n    return objectHash(this.trainingDataJson);\n  }\n\n  nextTrainBatch(batchSize :number) {\n    return this.nextBatch(batchSize, this.trainingIndices);\n  }\n\n  nextTestBatch(batchSize :number) {\n    return this.nextBatch(batchSize, this.testingIndices);\n  }\n\n  nextBatch(batchSize :number, indices :Uint32Array) {\n    const batchImagesArray = new Float32Array(batchSize * this.imageSize);\n    const batchLabelsArray = new Uint8Array(batchSize * this.numClasses);\n    console.assert(this.numClasses > 0);\n\n    for (let i = 0; i < batchSize; i++) {\n      // Cycle through indices if we exceed the range, this repeats examples, I hope that's fine ¯\\_(ツ)_/¯\n      const idx = indices[i % indices.length];\n\n      const startIndexData = idx * this.imageSize;\n      const image =\n          this.bigFatDataArray.slice(startIndexData, startIndexData + this.imageSize);\n      batchImagesArray.set(image, i * this.imageSize);\n\n      const startIndexLabels = idx * this.numClasses;\n      const label =\n        this.bigFatLabelArray.slice(startIndexLabels, startIndexLabels + this.numClasses);\n      batchLabelsArray.set(label, i * this.numClasses);\n    }\n\n    const xs = tf.tensor2d(batchImagesArray, [batchSize, this.imageSize]);\n    const labels = tf.tensor2d(batchLabelsArray, [batchSize, this.numClasses]);\n\n    // are the labels ok?\n    // const labelStrings = [];\n    // for (let i = 0; i < batchLabelsArray.length; i+= this.numClasses) {\n    //   const slice = batchLabelsArray.slice(i, i + 4);\n    //   const label = this.classNames[slice.indexOf(1)];\n    //   labelStrings.push(label);\n    // }\n    // console.log('labelStrings', labelStrings);\n\n    return {xs, labels};\n  }\n\n  // createModel() {\n  //   this.model = tf.sequential({\n  //     layers: [\n  //       tf.layers.conv1d({ kernelSize: 10, filters: 100, strides: 1, activation: 'relu', kernelInitializer: 'varianceScaling', inputShape: [this.imageWidth, this.imageHeight, 1] }),\n  //       tf.layers.conv1d({ kernelSize: 10, filters: 100, strides: 1, activation: 'relu' }),\n  //       tf.layers.maxPooling1d(3),\n  //       tf.layers.conv1d({ kernelSize: 10, filters: 160, strides: 1, activation: 'relu' }),\n  //       tf.layers.conv1d({ kernelSize: 10, filters: 160, strides: 1, activation: 'relu' }),\n  //       tf.layers.globalAveragePooling1d(),\n  //       tf.layers.dropout(0.5),\n  //       // Now we flatten the output from the 2D filters into a 1D vector to prepare\n  //       // it for input into our last layer. This is common practice when feeding\n  //       // higher dimensional data to a final classification output layer.\n  //       // tf.layers.flatten(),\n  //       // Our last layer is a dense layer which has 10 output units, one for each\n  //       // output class (i.e. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\n  //       // const NUM_OUTPUT_CLASSES = 10;\n  //       tf.layers.dense({ units: this.numClasses,  activation: 'softmax' }),// kernelInitializer: 'varianceScaling',\n\n  //       // Choose an optimizer, loss function and accuracy metric,\n  //     // // then compile and return the model\n  //     // const optimizer = tf.train.adam();\n  //     // this.model.compile({\n  //     //     optimizer: optimizer,\n  //     //     loss: 'categoricalCrossentropy',\n  //     //     metrics: ['accuracy'],\n  //     // });\n  //     ]\n  //   });\n\n  //   // const IMAGE_WIDTH = this.imageWidth;\n  //   // const IMAGE_HEIGHT = this.imageHeight;\n  //   // const IMAGE_CHANNELS = 1;\n\n  //   // In the first layer of out convolutional neural network we have\n  //   // to specify the input shape. Then we specify some paramaters for\n  //   // the convolution operation that takes place in this layer.\n  //   // https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D\n  //   // width is timesteps, height is accel[x,y,z]+gyro[x,y,z] (6)\n  //   // this.model.add(tf.layers.conv2d({\n  //   //     inputShape: [this.imageWidth, this.imageHeight, 1],\n  //   //     kernelSize: [3, 1],\n  //   //     filters: 8,\n  //   //     strides: 1,\n  //   //     activation: 'relu',\n  //   //     kernelInitializer: 'varianceScaling'\n  //   // }));\n\n  //   // this.model.add(tf.layers.conv1d({\n  //   //   inputShape: [this.imageWidth, this.imageHeight, 1],\n  //   //   // kernelSize: [6, 10],\n  //   //   kernelSize: 10,\n  //   //   filters: 100,\n  //   //   strides: 1,\n  //   //   activation: 'relu',\n  //   //   kernelInitializer: 'varianceScaling'\n  //   // }));\n\n  //   // this.model.add(tf.layers.conv1d({\n  //   //   kernelSize: 10,\n  //   //   filters: 100,\n  //   //   strides: 1,\n  //   //   activation: 'relu',\n  //   //   // kernelInitializer: 'varianceScaling'\n  //   // }));\n\n\n  //   // this.model.add(tf.layers.maxPooling1d(3));\n\n  //   // this.model.add(tf.layers.conv1d({\n  //   //   kernelSize: 10,\n  //   //   filters: 160,\n  //   //   strides: 1,\n  //   //   activation: 'relu',\n  //   // }));\n\n  //   // this.model.add(tf.layers.conv1d({\n  //   //   kernelSize: 10,\n  //   //   filters: 160,\n  //   //   strides: 1,\n  //   //   activation: 'relu',\n  //   // }));\n\n  //   // this.model.add(tf.layers.globalAveragePooling1d());\n\n\n  // //   // The MaxPooling layer acts as a sort of downsampling using max values\n  // //   // in a region instead of averaging.\n  // //   this.model.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\n\n  //   // Repeat another conv2d + maxPooling stack.\n  //   // Note that we have more filters in the convolution.\n  //   // this.model.add(tf.layers.conv2d({\n  //   //     kernelSize: 3,\n  //   //     filters: 16,\n  //   //     strides: 1,\n  //   //     activation: 'relu',\n  //   //     kernelInitializer: 'varianceScaling'\n  //   // }));\n  // //   this.model.add(tf.layers.conv1d({\n  // //     inputShape: [this.imageWidth, this.imageHeight, 1],\n  // //     kernelSize: [6, 10],\n  // //     filters: 100,\n  // //     strides: 1,\n  // //     activation: 'relu',\n  // //     kernelInitializer: 'varianceScaling'\n  // // }));\n\n\n  // //   this.model.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\n\n\n\n  //   // Now we flatten the output from the 2D filters into a 1D vector to prepare\n  //   // it for input into our last layer. This is common practice when feeding\n  //   // higher dimensional data to a final classification output layer.\n  //   // this.model.add(tf.layers.flatten());\n\n  //   // // Our last layer is a dense layer which has 10 output units, one for each\n  //   // // output class (i.e. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\n  //   // // const NUM_OUTPUT_CLASSES = 10;\n  //   // this.model.add(tf.layers.dense({\n  //   //     units: this.numClasses,\n  //   //     kernelInitializer: 'varianceScaling',\n  //   //     activation: 'softmax'\n  //   // }));\n\n\n  //   // Choose an optimizer, loss function and accuracy metric,\n  //   // then compile and return the model\n  //   const optimizer = tf.train.adam();\n  //   this.model.compile({\n  //       optimizer: optimizer,\n  //       loss: 'categoricalCrossentropy',\n  //       metrics: ['accuracy'],\n  //   });\n\n  //   return this.model;\n  // }\n\n  get classNames() {\n    return this._labels;\n  }\n\n  get gestureNames() {\n    return this.classNames;\n  }\n\n  get numClasses() {\n    return this._labels.length;\n  }\n\n  get imageWidth() {\n    return this._streams.length; // 6\n  }\n\n  get width() {\n    return this.imageWidth;\n  }\n\n  get imageHeight() {\n    return this.timeSteps\n  }\n\n  get height() {\n    return this.imageHeight;\n  }\n\n  get imageSize() {\n    return this.imageWidth * this.imageHeight;\n  }\n\n  get timeSteps() {\n    return this._timesteps;\n  }\n\n  get trainingExampleCount() {\n    return this.trainingIndices.length;\n  }\n\n  get testingExampleCount() {\n    return this.testingIndices.length;\n  }\n\n  // Load a single gesture JSON blob\n  // async loadGesture(url) {\n  //   const response = await fetch(url);\n  //   var sensorJson :SensorJson = await response.json();\n\n\n  //   // The original format is very inefficient.\n  //   // TODO should just store this format from the beginning\n  //   let length = sensorJson.accelerometer.length;\n  //   const accelerometer = {\n  //     x: new Float32Array(length),\n  //     y: new Float32Array(length),\n  //     z: new Float32Array(length),\n  //     t: new Int32Array(length),\n  //   };\n  //   length = sensorJson.gyroscope.length;\n  //   const gyroscope = {\n  //     x: new Float32Array(length),\n  //     y: new Float32Array(length),\n  //     z: new Float32Array(length),\n  //     t: new Int32Array(length),\n  //   };\n  //   for (let i = 0; i < sensorJson.accelerometer.length; i++) {\n  //     accelerometer.x[i] = sensorJson.accelerometer[i].x;\n  //     accelerometer.y[i] = sensorJson.accelerometer[i].y;\n  //     accelerometer.z[i] = sensorJson.accelerometer[i].z;\n  //     accelerometer.t[i] = sensorJson.accelerometer[i].t;\n  //   }\n  //   for (let i = 0; i < sensorJson.gyroscope.length; i++) {\n  //     gyroscope.x[i] = sensorJson.gyroscope[i].x;\n  //     gyroscope.y[i] = sensorJson.gyroscope[i].y;\n  //     gyroscope.z[i] = sensorJson.gyroscope[i].z;\n  //     gyroscope.t[i] = sensorJson.gyroscope[i].t;\n  //   }\n  //   return {accelerometer, gyroscope};\n  // }\n\n  /**\n   *\n  var x = {\n    x: [2, 3, 4, 5],\n    y: [16, 5, 11, 9],\n    mode: 'lines',\n    name: 'x'\n  };\n\n  var y = {\n    x: [1, 2, 3, 4],\n    y: [12, 9, 15, 12],\n    mode: 'lines',\n    name: 'y'\n  };\n\n  var data = [ x, y, z ]\n\n   */\n//   convertToPlotlyRaw(exampleBlob, gestureName) {\n//     const example = exampleBlob.data;\n//     const fileName = exampleBlob.url;\n\n//     return sensorNames.map((sensorName) => {\n//       const plotlyInitObj = {\n//         data: [],\n//         layout: {\n//           margin: {l:10,r:10,t:20,b:10},\n//           title: {\n//             text: fileName,\n//             font: {size: 8},\n//           },\n//           showlegend: false,\n//           xaxis: {\n//             showticklabels: false,\n//           },\n//           yaxis: {\n//             showticklabels: false,\n//           }\n//         },\n//         config: {\n//           responsive: true,\n//           displayModeBar: false,\n//         },\n\n//       };\n//       axes.forEach((axis) => {\n//         const plot = {\n//           x:[], //These are plotly values, not the sensor x,y values\n//           y:[],\n//           mode: 'lines',\n//           name: `${gestureName}:${sensorName}:${axis}:${fileName}`,\n//         }\n//         example[sensorName][axis].forEach((sensorDataPoint, sensorDataPointIndex) => {\n//           plot.x.push(example[sensorName].t[sensorDataPointIndex]);\n//           plot.y.push(sensorDataPoint);\n//         });\n//         plotlyInitObj.data.push(plot);\n//       });\n\n//       return plotlyInitObj;\n//     });\n//   }\n\n//   getExamplesPlotlyRaw() {\n//     const result = {};\n//     Object.keys(this.data).forEach((gestureName) => {\n//       result[gestureName] = this.data[gestureName].map(example => this.convertToPlotlyRaw(example, gestureName));\n//     });\n//     return result;\n//   }\n\n  // these are the slices of the huge data array and label array\n//   convertToPlotlyProcessed(imageArray, labelName, imageIndex) {\n//     const timeSteps = Array.apply(null, {length: this.timeSteps}).map(Number.call, Number);\n//     let sensorArrayIndex = 0;\n//     return sensorNames.map((sensorName) => {\n\n//       const plotlyInitObj = {\n//         data: [],\n//         layout: {\n//           margin: {l:10,r:10,t:20,b:10},\n//           title: {\n//             text: `${labelName}_${imageIndex}`,\n//             font: {size: 8},\n//           },\n//           showlegend: false,\n//           xaxis: {\n//             showticklabels: false,\n//           },\n//           yaxis: {\n//             showticklabels: false,\n//           }\n//         },\n//         config: {\n//           responsive: true,\n//           displayModeBar: false,\n//         },\n\n//       };\n\n//       axes.forEach((axis) => {\n//         const plot = {\n//           x   : [], //These are plotly values, not the sensor x,y values\n//           y   : [],\n//           mode: 'lines',\n//           name: `${labelName}:${sensorName}:${axis}:${imageIndex}`,\n//         };\n\n//         const offset = sensorArrayIndex * this.timeSteps;\n//         plot.y = imageArray.slice(offset, offset + this.timeSteps);\n//         plot.x = timeSteps;\n\n//         sensorArrayIndex++;\n\n//         plotlyInitObj.data.push(plot);\n//       });\n\n//       return plotlyInitObj;\n//     });\n//   }\n\n//   getExamplesPlotlyProcessed() {\n//     // this is very different from the raw, because we have to extract the data and labels out\n//     // but that's the point: verification that the processing is ok\n//     const result = {};\n//     Object.keys(this.data).forEach((labelName) => {\n//       result[labelName] = [];\n//     });\n\n//     const totalExampleCount = Object.keys(this.data).reduce((cur, label) => cur + this.data[label].length, 0);\n//     const exampleIndices = Array.apply(null, {length: totalExampleCount}).map(Number.call, Number);\n//     exampleIndices.forEach(exampleIndex => {\n//       const labelArray = this.bigFatLabelArray.slice(exampleIndex * this.numClasses, (exampleIndex * this.numClasses) + this.numClasses);\n//       const exampleArray = this.bigFatDataArray.slice(exampleIndex * this.imageSize, (exampleIndex * this.imageSize) + this.imageSize);\n\n//       // validation: check label exists\n//       if (labelArray.indexOf(1) === -1) {\n//         console.log(`No label: ${labelArray}`);\n//         throw `No label: ${labelArray}`;\n//       }\n\n//       const labelName = this.classNames[labelArray.indexOf(1)];\n\n//       // validation: check that there is only one label\n//       if (labelArray.reduce(count, 0) != 1) {\n//         throw `${labelName} has more than one label: ${labelArray}`;\n//       }\n\n//       const plotData = this.convertToPlotlyProcessed(exampleArray, labelName, exampleIndex);\n\n//       result[labelName].push(plotData);\n//     });\n\n//     return result;\n//   }\n\n\n  allNonTimeValues(iterateFunc :(dataPoint :number, dataPointIndex :number, data :Float32Array, stream:string, exampleIndex:number, gesturename:string)=>void) {// iterateFunc: (value, index, array, coordinate<x|y|z>, sensor <accelerometer|gyroscope>, exampleIndex, label)\n    this.allNonTimeSensorStreams((sensorArray :Float32Array, sensorname:string, exampleIndex:number, gestureName:string) => {\n        sensorArray.forEach((sensorDataPoint :number, sensorDataPointIndex :number) => {\n            iterateFunc(sensorDataPoint, sensorDataPointIndex, sensorArray, sensorname, exampleIndex, gestureName);\n        });\n    });\n  }\n\n//   allTimes(iterateFunc :(data :IMUSensorExample, sensorname:string, exampleIndex:number, gesturename:string)=>void) {// iterateFunc: (value, array, index, 't', sensor <accelerometer|gyroscope>, exampleIndex, label)\n//     this.allSensorStreams((sensorArrays, sensor, exampleIndex, gestureName) => {\n//       sensorArrays.t.forEach((sensorDataPoint, sensorDataPointIndex) => {\n//         iterateFunc(sensorDataPoint, sensorArrays, sensorDataPointIndex, 't', sensor, exampleIndex, gestureName);\n//       });\n//     });\n//   }\n\n  allSensorStreams(iterateFunc :(data :Float32Array, sensorname:string, exampleIndex:number, gesturename:string)=>void) { // iterateFunc: (object of xyzt arrays of sensor data points, sensor <ax ay etc>, exampleIndex, label)\n    Object.keys(this.data).forEach((gestureName :string) => {\n      const examples = this.data[gestureName];\n      examples.forEach((exampleBlob, exampleIndex) => {\n        const example = exampleBlob.data;\n        this._streams.forEach((sensorName :string) => {\n          iterateFunc(example[sensorName], sensorName, exampleIndex, gestureName);\n        });\n      });\n    });\n  }\n\n  allNonTimeSensorStreams(iterateFunc :(data :Float32Array, sensorname:string, exampleIndex:number, gesturename:string)=>void) { // iterateFunc: (object of xyzt arrays of sensor data points, sensor <ax ay etc>, exampleIndex, label)\n    Object.keys(this.data).forEach((gestureName :string) => {\n      const examples = this.data[gestureName];\n      examples.forEach((exampleBlob, exampleIndex) => {\n        const example = exampleBlob.data;\n        this._streams.forEach((sensorName :string) => {\n            if (sensorName.endsWith('t')) {\n                return;\n            }\n          iterateFunc(example[sensorName], sensorName, exampleIndex, gestureName);\n        });\n      });\n    });\n  }\n\n  allExamples(iterateFunc :(data :SensorSeries, exampleIndex:number, gesturename:string)=>void) { // iterateFunc: (object of xyzt arrays of sensor data points, sensor <ax ay etc>, exampleIndex, label)\n    Object.keys(this.data).forEach((gestureName :string) => {\n      const examples = this.data[gestureName];\n      examples.forEach((exampleBlob, exampleIndex) => {\n        const example = exampleBlob.data;\n        iterateFunc(example, exampleIndex, gestureName);\n      });\n    });\n  }\n\n  allExamplesForLabel(label :string) { // iterateFunc: (object of xyzt arrays of sensor data points, sensor <ax ay etc>, exampleIndex, label)\n    return this.data[label];\n  }\n\n  /**\n   * {\n  \"accelerometer\": [\n    {\n      \"x\": -8.828042030334473,\n      \"y\": 3.8751418590545654,\n      \"z\": 6.777905464172363,\n      \"t\": 1552805816650\n    },\n   */\n  getMaxValue() {\n    let max = 0;\n    const iter = (val :number) => {\n      max = Math.max(Math.abs(val), max);\n    }\n    this.allNonTimeValues(iter);\n    return max;\n  }\n\n  // Divide by the absolute max so all values [-1,1]\n  // For each series label:\n  //   - get the min+max over all examples\n  //   - normalize each series for the specific series min+max [-1,1]\n  //   - this way you don't need to then normalize between series, which\n  //     ruins signals where some signals have a bigger baseline variance\n  normalize() {\n    console.log('    normalizing...');\n\n    const nonTimeSensorStreams = this._streams.filter(s => !s.endsWith('t'));\n    // initialize the ranges map\n    nonTimeSensorStreams.forEach(stream => this.ranges[stream] = {min:Number.MAX_VALUE, max:Number.MIN_VALUE, absmax:Number.MIN_VALUE});\n    this.allNonTimeSensorStreams((sensorArray :Float32Array, sensorname:string, exampleIndex:number, gestureName:string) => {\n        sensorArray.forEach((sensorDataPoint :number) => {\n          this.ranges[sensorname].min = Math.min(this.ranges[sensorname].min, sensorDataPoint);\n          this.ranges[sensorname].max = Math.max(this.ranges[sensorname].max, sensorDataPoint);\n        });\n    });\n\n    // We use the absolute max for normalizing\n    nonTimeSensorStreams.forEach(stream => this.ranges[stream].absmax = Math.max(this.ranges[stream].max, Math.abs(this.ranges[stream].min)));\n\n    this.allNonTimeSensorStreams((sensorArray :Float32Array, sensorname:string, exampleIndex:number, gestureName:string) => {\n      const sensorMax = this.ranges[sensorname].absmax;\n      sensorArray.forEach((_ :number, index:number) => {\n        sensorArray[index] = sensorArray[index] / sensorMax;\n      });\n  });\n\n\n    // let max = this.getMaxValue();\n    // this._maxAbsoluteValue = max;\n    console.log(`        this.ranges=${JSON.stringify(this.ranges)}`);\n\n\n\n    // this.allNonTimeValues((val, index, arr) => {\n    //   arr[index] = val / max;\n    // });\n  }\n\n  // Time starts at zero (absolute time is recorded)\n  zeroTime() {\n    console.log('    zeroing time...');\n    // iterateFunc: (arrays of sensor data points, sensor <accelerometer|gyroscope>, exampleIndex, label)\n    this.allExamples((example) => {\n        const timeStreams = this._streams.filter(s => s.endsWith('t'));\n\n        // get the earliest time and make that zero and adjust\n        let minTime :number = Number.MAX_VALUE;\n        timeStreams.forEach(timestream => {\n          example[timestream].forEach((sensorDataPoint:number, i) => minTime = Math.min(minTime, sensorDataPoint));\n        });\n\n        // make that zero and adjust\n        timeStreams.forEach(timestream => {\n            example[timestream].forEach((time :number, index :number) => {\n                example[timestream][index] = time - minTime;\n            });\n        });\n\n\n\n        // timeStreams.forEach(timestream => {\n        //     sensorArrays[timestream].forEach((time:number, index:number) => {\n        //         sensorArrays.at[index] = time - minTime;\n        //     });\n        // });\n\n\n\n    //   sensorArrays.at.forEach((time, index) => {\n    //     sensorArrays.at[index] = time - minTime;\n    //   });\n    //   sensorArrays.gt.forEach((time, index) => {\n    //     sensorArrays.gt[index] = time - minTime;\n    //   });\n    });\n  }\n\n  // removeEmptyExamples() {\n  //   trimToLongestNonZeroGesture()\n  // }\n\n  // Time starts at zero (absolute time is recorded)\n  trimToLongestNonZeroGesture() {\n    console.log('    getting longest non-default gesture...');\n    let max = 0;\n    let maxAll = 0;\n    const streamsWithoutTime = this._streams.filter(s => !s.endsWith('t'));\n    this.allExamples((example, _, gesture) => {\n      // control examples don't count\n      if (!this.trainingDataJson.controlLabels || !this.trainingDataJson.controlLabels.includes(gesture)) {\n        // just grab the length of the first stream, assume all the same length\n        // console.log('example', example);\n        // sometimes I get an empty object, what's up with that?\n        if (!example[this._streams[0]]) {\n          return;\n        }\n        let lastNonZero = example[this._streams[0]].length - 1;\n        for ( ; lastNonZero >= 0;lastNonZero--) {\n            // if any stream (ignoring time) contain a non-zero value, this is the end of the actual stream\n            if (streamsWithoutTime.filter((stream) => example[stream][lastNonZero] != 0).length > 0) {\n                break;\n            }\n        }\n        max = Math.max(max, lastNonZero + 1);\n      }\n      maxAll = Math.max(maxAll, example[this._streams[0]].length);\n    });\n    console.log(`        [max=${max}] [maxAll=${maxAll}]...trimming...`);\n    this.allExamples((example, _, gesture) => {\n        this._streams.forEach((stream) => {\n            if (!example[stream]) {\n              return;\n            }\n            example[stream] = example[stream].slice(0, max);\n        })\n    });\n    // this.allSensorStreams((sensorArrays) => {\n    //   this._streams.forEach((stream) => {\n    //       if (!sensorArrays[stream]) {\n    //           console.log(`stream=${stream} missing from sensorArrays=${sensorArrays}`);\n    //       }\n    //     sensorArrays[stream] = sensorArrays[stream].slice(0, max);\n    //   });\n    // });\n    maxAll = 0;\n    this.allSensorStreams((sensorArray) => {\n      maxAll = Math.max(maxAll, sensorArray.length);\n    });\n    console.log(`        all arrays now [max=${maxAll}]`);\n  }\n\n  // All examples must be the same length\n  // This will end up being the max input length?\n  extend() {\n    console.log('    extending...');\n    let maxTimeSteps = 0;\n    const timeDeltas :number[] = [];\n    // this.allExamples((example, _, gesture) => {\n    // });\n    this.allSensorStreams((sensorArray) => {\n      maxTimeSteps = Math.max(maxTimeSteps, sensorArray.length);\n      sensorArray.forEach((point :number, index:number) => {\n        if (index == 0) {\n          return;\n        }\n        timeDeltas.push(point - sensorArray[index - 1]);\n      });\n    });\n    // This probably won't matter but we're gonna make timesteps monotonic int counters,\n    // ignoring actual millisecond differences, some detail will be lost but the data needs\n    // to be smoothed anyway. Smoothing, if it happends should be done before this step.\n    // const meanTimeDelta = Math.floor(timeDeltas.reduce((curTotal, val) => curTotal + val) / timeDeltas.length);\n    // this.allSensorStreams((sensorArrays) => {\n    //   if (sensorArrays.t.length < maxTimeSteps) {\n    //     let newTimeIndex = sensorArrays.t.length;\n    //     axes.concat(['t']).forEach((axis) => {\n    //       const current = sensorArrays[axis];\n    //       sensorArrays[axis] = new Float32Array(maxTimeSteps);\n    //       sensorArrays[axis].set(current);\n    //     });\n    //     while (newTimeIndex < maxTimeSteps) {\n    //       sensorArrays.t[newTimeIndex] = meanTimeDelta * newTimeIndex;\n    //       newTimeIndex++;\n    //     }\n    //   }\n    // });\n    this._timesteps = maxTimeSteps;\n    console.log(`        this.timeSteps=${this.timeSteps}`);\n  }\n\n  // processExample(example :IMUSensorExample | null | undefined) :any {\n  //   if (!example) {\n  //     throw \"processExample: missing example\";\n  //   }\n\n  //   // remove the time arrays, they aren't really adding much I think\n  //   Object.keys(example).forEach(stream => {\n  //       if (stream.endsWith('t')) {\n  //           delete example[stream];\n  //       }\n  //   })\n  // }\n\n  processPrediction(example :SensorSeries) :Float32Array {\n    if (!example) {\n      throw \"processExample: missing example\";\n    }\n    if (this._timesteps === 0) {\n      throw 'processPrediction but this._timesteps === 0';\n    }\n\n\n    // this.processExample(example);\n\n    // trim/extend so same timeSteps\n    Object.keys(example).forEach(stream => {\n      if (example[stream].length > this._timesteps) {\n        example[stream] = example[stream].slice(0, this._timesteps);\n      }\n      // ensure length == timeSteps\n      if (example[stream].length < this._timesteps) {\n        const next = new Float32Array(this._timesteps);\n        next.set(example[stream])\n        example[stream] = next;\n      }\n    });\n\n    // normalize over max over all training examples\n    Object.keys(example).forEach((stream :string) => {\n      const sensorMax = this.ranges[stream].absmax;\n      example[stream].forEach((val :number, index:number, arr) => {\n        arr[index] = val / sensorMax;\n      });\n    });\n\n    const dataArray = new Float32Array(1 * this.imageSize);\n\n    let sensorArrayIndex = 0;\n    // sensorNames.forEach((sensor) => {\n    this._streams.forEach((stream) => {\n      const offset = sensorArrayIndex * this.timeSteps;\n      dataArray.set(example[stream], offset);\n      // next sensor stream\n      sensorArrayIndex++;\n    });\n\n    return dataArray;\n  }\n\n  async load() {\n    console.log(`Begin loading examples from ${this.trainingDataJson.examples.length} gestures and converting to float arrays...`);\n    this.data = {};// <label, [{url:string,data:{x|y|z:Float32Array,t:Int32Array}}]>\n    // Load all the JSON blobs async\n    const data = this.data;\n    this._streams = [];\n    const axesSet :{[key:string]:boolean}= {};\n    console.log(this?.trainingDataJson?.examples[0])\n    this.trainingDataJson.examples.forEach((example :TrainingDataPoint) => {\n        const label = example.label;\n        if (!data[label]) {\n            data[label] = [];\n        }\n        const jsonData: SensorSeries = sensorSeriesDecode(example.data.series);\n        Object.keys(jsonData!).forEach(a => axesSet[a] = true);\n        const processedExample :SensorJson = {data:jsonData, url:(example.name || example.url) as string};\n        data[label].push(processedExample);\n    });\n\n    this._streams = Object.keys(axesSet);\n    this._streams.sort();\n    console.log('this._streams', this._streams);\n    this._labels = Object.keys(this.data);\n    console.log('this._labels', this._labels);\n    this._labels.sort();\n    console.log(`labels: [  ${this._labels.join('  |  ')}  ]`);\n    console.log('    done loading raw gesture data, begin preprocessing...');\n    this.trimToLongestNonZeroGesture();\n    this.normalize();\n    const normalizedMax = this.getMaxValue();\n    console.log(`        normalizedMax=${normalizedMax}`);\n    if (normalizedMax != 1.0) {\n      throw 'Failed to normalize';\n    }\n    this.zeroTime();\n    this.extend();\n\n\n    console.log('    converting to large combined float arrays...');\n    // Building data and label arrays\n    const totalExampleCount = Object.keys(this.data).reduce((cur, label) => cur + this.data[label].length, 0);\n    this.bigFatDataArray = new Float32Array(totalExampleCount * this.imageSize);\n    this.bigFatLabelArray = new Uint8Array(totalExampleCount * this.numClasses);\n\n    console.log(`        totalExampleCount=${totalExampleCount} imageSize=${this.imageSize} numClasses=${this.numClasses}`);\n    console.log(`            width=${this.width} height=${this.height}`);\n\n    let arrayIndex = 0;\n    this.classNames.forEach((gestureName, gestureIndex) => {\n\n      const examples = this.data[gestureName];\n\n      // Create the FloatArrays from the sensor stream data\n      examples.forEach((exampleBlob) => {\n        // Set the label value\n        this.bigFatLabelArray[(arrayIndex * this.numClasses) + gestureIndex] = 1;\n        // get the example and convert to a 6*timestep \"image\"\n        const example = exampleBlob.data; // {accelerometer:{x|y|z: Float32Array,t:Int32Array}, gyroscope:...}\n        // Copy the data into arrays\n        // There are six of these ((acc+gyro)*(x+y+z))\n        let sensorArrayIndex = 0;\n        // sensorNames.forEach((sensor) => {\n          this._streams.forEach((stream) => {\n            const offset = (arrayIndex * this.imageSize) + (sensorArrayIndex * this.timeSteps);\n            // console.log(`            offset=${offset} sensor=${sensor} axis=${axis}`);\n            this.bigFatDataArray.set(example[stream], offset);\n            // next sensor stream\n            sensorArrayIndex++;\n          });\n        // });\n        // next complete gesture example\n        arrayIndex++;\n      });\n    });\n\n    // Now we have all the examples and labels in byte arrays. Training and testing involves\n    // quickly getting arrays of the data and labels. To speed this up, randomize and combine\n    // arrays into two big array buffers, testing and training.\n    // Providing training/test arrays will be just selecting from the right shuffled indices\n    // and copying the data buffers\n\n    const allExampleIndicesShuffled = tf.util.createShuffledIndices(totalExampleCount);\n    const numExamplesTraining = Math.floor(proportionTrainingSamples * totalExampleCount);\n    // const numExamplesTesting = totalExampleCount - numExamplesTraining;\n    this.trainingIndices = allExampleIndicesShuffled.slice(0, numExamplesTraining);\n    this.testingIndices = allExampleIndicesShuffled.slice(numExamplesTraining);\n\n    // console.log('this.trainingIndices', this.trainingIndices);\n    // console.log('this.testingIndices', this.testingIndices);\n    // this.trainImages = new Float32Array(numExamplesTraining * this.imageSize);\n    // this.trainLabels = new Uint8Array(numExamplesTraining * this.numClasses);\n    // this.testImages = new Float32Array(numExamplesTesting * this.imageSize);\n    // this.testLabels = new Uint8Array(numExamplesTesting * this.numClasses);\n    // for (let i = 0; i < this.dataArrays.length; i++) {\n    //   const label = new Uint8Array(this.numClasses);\n    //   label[]\n    // }\n\n\n\n    // //TODO I NEED THESE FOR THE FUNCTIONS BELOW\n\n    // //Done\n    // // this.datasetLabels = new Uint8Array(await labelsResponse.arrayBuffer());\n    // console.log(`this.datasetLabels=${this.datasetLabels}`);\n\n    // const indicesTraining = allExampleIndicesShuffled.slice(0, numExamplesTraining);\n    // const indicesTesting = allExampleIndicesShuffled.slice(numExamplesTraining);\n    // this.trainIndices = indicesTraining;\n    // this.testIndices = indicesTesting;\n\n    // // Create shuffled indices into the train/test set for when we select a\n    // // random dataset element for training / validation.\n    // this.trainIndices = tf.util.x  (NUM_TRAIN_ELEMENTS);\n    // this.testIndices = tf.util.createShuffledIndices(NUM_TEST_ELEMENTS);\n    //DONE ABOVE\n    // // Slice the the images and labels into train and test sets.\n    // this.trainImages =\n    //     this.datasetImages.slice(0, IMAGE_SIZE * NUM_TRAIN_ELEMENTS);\n    // this.testImages = this.datasetImages.slice(IMAGE_SIZE * NUM_TRAIN_ELEMENTS);\n    // this.trainLabels =\n    //     this.datasetLabels.slice(0, NUM_CLASSES * NUM_TRAIN_ELEMENTS);\n    // this.testLabels =\n    //     this.datasetLabels.slice(NUM_CLASSES * NUM_TRAIN_ELEMENTS);\n  }\n}\n","/**\n * @license\n * Private wand license.\n */\n\nimport {TrainingData} from './TrainingData';\nimport * as tf from '@tensorflow/tfjs';\nimport * as tfvis from '@tensorflow/tfjs-vis';\n\n// import { Drawable } from '@tensorflow/tfjs-vis';\n\nconst BATCH_SIZE = 32;\nconst EPOCHS = 10;\n\nexport class Trainer {\n    _data:TrainingData;\n    _model:any;\n    classNames :string[] = [];\n\n  constructor(data:TrainingData) {\n    this._data = data;\n    this.createModel();\n  }\n\n  get model() {\n    return this._model;\n  }\n\n  createModel() {\n    const data = this._data;\n    this.classNames = data.classNames;\n    // const filters1 = 64; // 100\n    // const filters2 = 10; // 160\n    // const kernelSize = 10; // 100\n    this._model = tf.sequential();\n    // this._model = tf.sequential({\n    //   layers: [\n    //     tf.layers.conv1d({ kernelSize: 10, filters: filters1, strides: 1, activation: 'relu', kernelInitializer: 'varianceScaling', inputShape: [data.imageWidth, data.imageHeight] }),\n    //     tf.layers.conv1d({ kernelSize: 10, filters: filters1, strides: 1, activation: 'relu' }),\n    //     tf.layers.maxPooling1d({poolSize:3}),\n    //     tf.layers.conv1d({ kernelSize: 10, filters: filters2, strides: 1, activation: 'relu' }),\n    //     tf.layers.conv1d({ kernelSize: 10, filters: filters2, strides: 1, activation: 'relu' }),\n    //     tf.layers.globalAveragePooling1d(),\n    //     tf.layers.dropout({rate:0.5}),\n\n        // Now we flatten the output from the 2D filters into a 1D vector to prepare\n        // it for input into our last layer. This is common practice when feeding\n        // higher dimensional data to a final classification output layer.\n        // tf.layers.flatten(),\n\n    //     // Our last layer is a dense layer which has 1 output unit for each \"gesture\" including the null gesture\n    //     tf.layers.dense({ units: data.numClasses,  activation: 'softmax' }),// kernelInitializer: 'varianceScaling',\n    //   ]\n    // });\n\n    // const width = data.imageWidth;\n    // const height = data.imageHeight;\n    // console.log('width', width);\n    // console.log('height', height);\n\n    this._model.add(tf.layers.conv1d({ filters: 100, kernelSize: 7, strides: 1, activation: 'relu', kernelInitializer: 'varianceScaling', inputShape: [data.height, data.width] }));//, batchSize: BATCH_SIZE\n    this._model.add(tf.layers.conv1d({ filters: 100, kernelSize: 7, strides: 1, activation: 'relu' }));\n    this._model.add(tf.layers.conv1d({ filters: 100, kernelSize: 7, strides: 1, activation: 'relu' }));\n    this._model.add(tf.layers.maxPooling1d({poolSize:3}));\n    this._model.add(tf.layers.conv1d({ filters: 100, kernelSize: 7, strides: 1, activation: 'relu' }));\n    // this._model.add(tf.layers.conv1d({ filters: 64, kernelSize: 7, strides: 1, activation: 'relu' }));\n    this._model.add(tf.layers.globalAveragePooling1d());\n    this._model.add(tf.layers.dropout({rate:0.5}));\n\n    // Now we flatten the output from the 2D filters into a 1D vector to prepare\n    // it for input into our last layer. This is common practice when feeding\n    // higher dimensional data to a final classification output layer.\n    tf.layers.flatten(),\n\n    // Our last layer is a dense layer which has 1 output unit for each \"gesture\" including the null gesture\n    this._model.add(tf.layers.dense({ units: data.classNames.length, activation: 'softmax' }));// kernelInitializer: 'varianceScaling',\n\n\n    this._model.summary();\n\n    // Choose an optimizer, loss function and accuracy metric,\n    // then compile and return the model\n    const optimizer = tf.train.adam();\n    this._model.compile({\n        optimizer: optimizer,\n        loss: 'categoricalCrossentropy',\n        metrics: ['accuracy'],\n    });\n\n    return this._model;\n  }\n\n  async train() {\n    const data = this._data;\n    const metrics = ['loss', 'val_loss', 'acc', 'val_acc'];\n    // const container = {\n    //     name: 'Model Training', styles: { height: '1000px' }\n    // };\n    const container = document.getElementById('Training') as any;\n    const fitCallbacks = tfvis.show.fitCallbacks(container, metrics);\n\n    // const TRAIN_DATA_SIZE = 120;\n    // const TEST_DATA_SIZE = 120;\n\n    // const totalExamples = data.trainingExampleCount\n\n    const trainingDataSize = data.trainingExampleCount * 2;\n    const [trainXs, trainYs] = tf.tidy(() => {\n        const d = data.nextTrainBatch(trainingDataSize);\n        return [\n          // d.xs.reshape([TRAIN_DATA_SIZE, data.imageWidth, data.imageHeight, 1]),\n          // d.xs.reshape([TRAIN_DATA_SIZE, data.imageHeight, data.imageWidth, 1]),\n          d.xs.reshape([trainingDataSize, data.imageHeight, data.imageWidth]),\n          d.labels\n        ];\n    });\n\n    // console.log('trainXs', trainXs);\n    // console.log('trainYs', trainYs);\n\n    const testingDataSize = data.testingExampleCount * 2;\n    const [testXs, testYs] = tf.tidy(() => {\n        const d = data.nextTestBatch(testingDataSize);\n        return [\n          // d.xs.reshape([TEST_DATA_SIZE, data.imageWidth, data.imageHeight, 1]),\n          // d.xs.reshape([TRAIN_DATA_SIZE, data.imageHeight, data.imageWidth, 1]),\n          d.xs.reshape([testingDataSize, data.imageHeight, data.imageWidth]),\n          d.labels\n        ];\n    });\n\n    const m = await this.model.fit(trainXs, trainYs, {\n        batchSize: BATCH_SIZE,\n        validationData: [testXs, testYs],\n        epochs: EPOCHS,\n        shuffle: true,\n        callbacks: fitCallbacks\n    });\n\n    this.model\n\n    // tfvis.show.modelSummary(container, m);\n    return m;\n  }\n}\n","import * as tf from \"@tensorflow/tfjs\";\nimport {PersistedModel, PersistedModelJson} from \"./types\";\nimport {base64encode, base64decode, SensorSeries, PredictionInput, PredictionResult} from \"./metaframe\";\n\n/**\n * Load and save tensorflow js models to JSON\n *\n * const modelJson = '{ ... }';\n * const handlerLoader = new JsonIOHandler(modelJson);\n * const model = await tf.loadLayersModel(handlerLoader);\n *\n * const handlerSaver = new JsonIOHandler();\n * const saveResult = model.save(handlerSaver);\n * const savedModelJson = handlerSaver.modelJson;\n */\n\n\nexport const jsonToModel = async (m : PersistedModelJson): Promise<PersistedModel> => {\n  const handlerLoader = new JsonIOHandler(m.model);\n  const loadedModel = await tf.loadLayersModel(handlerLoader);\n  const persistedModel: PersistedModel = Object.assign({}, m);\n  persistedModel.model = loadedModel;\n  return persistedModel;\n};\n\nexport const modelToJson = async (m : PersistedModel): Promise<PersistedModelJson> => {\n  const handlerSaver = new JsonIOHandler();\n  const saveResult = await m.model.save(handlerSaver);\n  // console.log(\"saveResult\", saveResult);\n  const persistedModelJson: PersistedModelJson = Object.assign({}, m);\n  persistedModelJson.model = handlerSaver.modelJson;\n  return persistedModelJson;\n};\n\nclass JsonIOHandler implements tf.io.IOHandler {\n  modelJson: any;\n  constructor(input? : any) {\n    this.modelJson = input;\n  }\n\n  async save(modelArtifact : tf.io.ModelArtifacts): Promise < tf.io.SaveResult > {\n    const saveResult: tf.io.SaveResult = {\n      modelArtifactsInfo: {\n        dateSaved: new Date(),\n        modelTopologyType: \"JSON\",\n        modelTopologyBytes: (modelArtifact.modelTopology as ArrayBuffer).byteLength,\n        // weightSpecsBytes: unknown,\n        weightDataBytes: (modelArtifact.weightData as ArrayBuffer).byteLength\n      }\n    };\n    try {\n      this.modelJson = Object.assign({}, modelArtifact);\n      this.modelJson.modelTopology = base64encode(modelArtifact.modelTopology as ArrayBuffer);\n      this.modelJson.weightData = base64encode(modelArtifact.weightData as ArrayBuffer);\n      return saveResult;\n    } catch (err) {\n      saveResult.errors = [`{err}`];\n      return saveResult;\n    }\n  }\n  async load(): Promise < tf.io.ModelArtifacts > {\n    console.log(\"JsonIOHandler.loading\");\n    const modelArtifacts: tf.io.ModelArtifacts = Object.assign({}, this.modelJson);\n    modelArtifacts.modelTopology = base64decode(this.modelJson.modelTopology as string);\n    modelArtifacts.weightData = base64decode(this.modelJson.weightData as string);\n    console.log(\"JsonIOHandler.modelArtifacts\", modelArtifacts);\n    return modelArtifacts;\n  }\n}\n\nexport const predict = async (\n  model: PersistedModel,\n  input: PredictionInput\n): Promise<[PredictionResult | undefined, Error | undefined]> => {\n  if (!model) {\n    return [\n      undefined,\n      new Error(\"Asked to predict sample but no model loaded\"),\n    ];\n  }\n  if (!input) {\n    return [\n      undefined,\n      new Error(\"Asked to predict but no input\"),\n    ];\n  }\n\n  if (!input.series) {\n    return [\n      undefined,\n      new Error(\"Asked to predict but input lacks 'series' field\"),\n    ];\n  }\n\n  const sampleFloatArrays: SensorSeries = input.series;\n  const processedSample = processPrediction(model, sampleFloatArrays);\n  const xs = tf.tensor3d(processedSample, [\n    1,\n    model.meta.prediction.imageHeight,\n    model.meta.prediction.imageWidth,\n  ]);\n  const prediction = (\n    model.model.predictOnBatch(xs) as tf.Tensor<tf.Rank>\n  ).dataSync();\n\n  let highest: number = 0;\n  let labelWithHighestScore: string = \"\";\n  let note:string|undefined;\n  const predictionMap: {\n    [key: string]: number;\n  } = {};\n\n  model.meta.prediction.classNames.forEach((value: string, index: number) => {\n    predictionMap[value] = prediction[index] as number;\n    if (prediction[index] > highest) {\n      highest = prediction[index];\n      labelWithHighestScore = value;\n    }\n  });\n\n  if (labelWithHighestScore !== \"_\" && predictionMap[labelWithHighestScore] < 0.45 && predictionMap[\"_\"]) {\n    note = `${labelWithHighestScore} -> _ because score < 0.45`;\n    labelWithHighestScore = \"_\";\n  }\n\n  const predictionJson: PredictionResult = {\n    prediction: labelWithHighestScore,\n    predictions: predictionMap,\n    requestId: input.requestId,\n    modelHash: model.meta.training.hash,\n    note,\n  };\n\n  return [predictionJson, undefined];\n\n  // document.getElementById(\"evaluation-result\")!.innerHTML = `\n  //     ${className} @${new Date().toISOString().split(\"T\")[1]}\n  //   `;\n\n  // if (isIframe()) metaframe.setOutput(\"prediction\", predictionJson);\n};\n\n\nexport const processPrediction = (\n  model: PersistedModel,\n  example: SensorSeries\n): Float32Array => {\n  if (!example) {\n    throw \"processExample: missing example\";\n  }\n  if (!model) {\n    throw \"processExample: missing model\";\n  }\n\n  // remove the time arrays, they aren't really adding much I think\n  Object.keys(example).forEach((stream: string) => {\n    if (stream.endsWith(\"t\")) {\n      delete example[stream];\n    }\n  });\n\n  const keys = Object.keys(example);\n  keys.sort();\n\n  console.log('keys', keys);\n\n  // TODO this is a hack, won't always work, but I can't remember right now\n  const timesteps = Math.max(\n    model.meta.prediction.imageHeight,\n    model.meta.prediction.imageWidth\n  );\n\n  // trim/extend so same timeSteps\n  keys.forEach((stream) => {\n\n    if (example?.[stream]?.length > timesteps) {\n      example[stream] = example[stream].slice(0, timesteps);\n    }\n    // ensure length == timeSteps\n    if (example?.[stream]?.length < timesteps) {\n      const next = new Float32Array(timesteps);\n      next.set(example[stream]);\n      example[stream] = next;\n    }\n  });\n\n  // normalize over max over all training examples\n  keys.forEach((stream: string) => {\n    example[stream].forEach(\n      (val: number, index: number, arr: Float32Array | Int32Array) => {\n        arr[index] = val / model!.meta.training.ranges[stream].absmax;\n      }\n    );\n  });\n\n  // put all data in a big array\n  // TODO can we just create the tensor3d here would it be faster?\n  const dataArray = new Float32Array(\n    1 * model.meta.prediction.imageWidth * model.meta.prediction.imageHeight\n  );\n  keys.forEach((stream, streamIndex) => {\n    const offset: number = streamIndex * timesteps;\n    dataArray.set(example[stream], offset);\n  });\n\n  return dataArray;\n};\n\n\n\n// const saveModel = async (m :PersistedModel, modelKey :string) :Promise<void> => {\n\n//     const handlerSaver = new JsonIOHandler();\n//     const saveResult = m.model.save(handlerSaver);\n//     const savedModelJson = handlerSaver.modelJson;\n\n//     await m.model.save(`indexeddb://${modelKey}`);\n//     return;\n//      await m.model.save(`localstorage://${modelKey}`);\n//      console.log('keys', localStorage.key);\n\n//      const model_metadata = localStorage.getItem(`tensorflowjs_models/${modelKey}/model_metadata`)!;\n//      const weight_data = localStorage.getItem(`tensorflowjs_models/${modelKey}/weight_data`)!;\n//      const weight_specs = localStorage.getItem(`tensorflowjs_models/${modelKey}/weight_specs`)!;\n//      const model_topology = localStorage.getItem(`tensorflowjs_models/${modelKey}/model_topology`)!;\n//      const info = localStorage.getItem(`tensorflowjs_models/${modelKey}/info`)!;\n\n//      const persistedModel :TFPersistedModelV1 = {\n//          version: 1,\n//          model: {model_metadata, weight_data, weight_specs, model_topology, info},\n//          classNames: m.classNames,\n//          imageHeight: m.imageHeight,\n//          imageWidth: m.imageWidth,\n//          maxAbsoluteRawValue: m.maxAbsoluteRawValue,\n//      }\n//      const modelString = JSON.stringify(persistedModel);\n\n//      localStorage.setItem(`models/${modelKey}`, modelString);\n\n//      if (isIframe()) metaframe.setOutput('model', modelString);\n// }\n\n// const loadModel = async (modelKey :string) :Promise<PersistedModel|undefined> => {\n//     return undefined;\n//      const modelString :string = await store.get(`models/${modelKey}`);\n\n//       const modelString = localStorage.getItem(`models/${modelKey}`);\n//      if (modelString) {\n//          const modelBlob :TFPersistedModelV1 = JSON.parse(modelString);\n//          Object.keys(modelBlob.model).forEach(key => localStorage.setItem(`tensorflowjs_models/${modelKey}/${key}`, modelBlob.model[key]))\n//          const model = await tf.loadLayersModel(`localstorage://${modelKey}`);\n//          const persistedModel :PersistedModel = {\n//              model,\n//              classNames: modelBlob.classNames,\n//              imageHeight: modelBlob.imageHeight,\n//              imageWidth: modelBlob.imageWidth,\n//              maxAbsoluteRawValue: modelBlob.maxAbsoluteRawValue,\n//          }\n//          updateMessage('Model ready', 'green');\n//          return persistedModel;\n//      }\n// }\n\n\n\n\n/*\nCopyright (c) 2011, Daniel Guerrero\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL DANIEL GUERRERO BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n/**\n * Uses the new array typed in javascript to binary base64 encode/decode\n * at the moment just decodes a binary base64 encoded\n * into either an ArrayBuffer (decodeArrayBuffer)\n * or into an Uint8Array (decode)\n *\n * References:\n * https://developer.mozilla.org/en/JavaScript_typed_arrays/ArrayBuffer\n * https://developer.mozilla.org/en/JavaScript_typed_arrays/Uint8Array\n */\n\n// export const Base64Binary = {\n// \t_keyStr : \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=\",\n\n// \t/* will return a  Uint8Array type */\n// \tdecodeArrayBuffer: function(input) {\n// \t\tvar bytes = (input.length/4) * 3;\n// \t\tvar ab = new ArrayBuffer(bytes);\n// \t\tthis.decode(input, ab);\n\n// \t\treturn ab;\n// \t},\n\n// \tremovePaddingChars: function(input){\n// \t\tvar lkey = this._keyStr.indexOf(input.charAt(input.length - 1));\n// \t\tif(lkey == 64){\n// \t\t\treturn input.substring(0,input.length - 1);\n// \t\t}\n// \t\treturn input;\n// \t},\n\n// \tdecode: function (input :string, arrayBuffer ?:ArrayBuffer) :Uint8Array {\n// \t get last chars to see if are valid\n// \t\tinput = this.removePaddingChars(input);\n// \t\tinput = this.removePaddingChars(input);\n\n// \t\tvar bytes = parseInt(\"\" + (input.length / 4) * 3, 10);\n\n// \t\tvar uarray;\n// \t\tvar chr1, chr2, chr3;\n// \t\tvar enc1, enc2, enc3, enc4;\n// \t\tvar i = 0;\n// \t\tvar j = 0;\n\n// \t\tif (arrayBuffer)\n// \t\t\tuarray = new Uint8Array(arrayBuffer);\n// \t\telse\n// \t\t\tuarray = new Uint8Array(bytes);\n\n// \t\tinput = input.replace(/[^A-Za-z0-9\\+\\/\\=]/g, \"\");\n\n// \t\tfor (i=0; i<bytes; i+=3) {\n// \t\t get the 3 octects in 4 ascii chars\n// \t\t\tenc1 = this._keyStr.indexOf(input.charAt(j++));\n// \t\t\tenc2 = this._keyStr.indexOf(input.charAt(j++));\n// \t\t\tenc3 = this._keyStr.indexOf(input.charAt(j++));\n// \t\t\tenc4 = this._keyStr.indexOf(input.charAt(j++));\n\n// \t\t\tchr1 = (enc1 << 2) | (enc2 >> 4);\n// \t\t\tchr2 = ((enc2 & 15) << 4) | (enc3 >> 2);\n// \t\t\tchr3 = ((enc3 & 3) << 6) | enc4;\n\n// \t\t\tuarray[i] = chr1;\n// \t\t\tif (enc3 != 64) uarray[i+1] = chr2;\n// \t\t\tif (enc4 != 64) uarray[i+2] = chr3;\n// \t\t}\n\n// \t\treturn uarray;\n// \t}\n// }\n\n// export function base64ArrayBuffer(arrayBuffer :ArrayBuffer) :string {\n//     var base64    = ''\n//     var encodings = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/'\n\n//     var bytes         = new Uint8Array(arrayBuffer)\n//     var byteLength    = bytes.byteLength\n//     var byteRemainder = byteLength % 3\n//     var mainLength    = byteLength - byteRemainder\n\n//     var a, b, c, d\n//     var chunk\n\n//      Main loop deals with bytes in chunks of 3\n//     for (var i = 0; i < mainLength; i = i + 3) {\n//        Combine the three bytes into a single integer\n//       chunk = (bytes[i] << 16) | (bytes[i + 1] << 8) | bytes[i + 2]\n\n//        Use bitmasks to extract 6-bit segments from the triplet\n//       a = (chunk & 16515072) >> 18  16515072 = (2^6 - 1) << 18\n//       b = (chunk & 258048)   >> 12  258048   = (2^6 - 1) << 12\n//       c = (chunk & 4032)     >>  6  4032     = (2^6 - 1) << 6\n//       d = chunk & 63                63       = 2^6 - 1\n\n//        Convert the raw binary segments to the appropriate ASCII encoding\n//       base64 += encodings[a] + encodings[b] + encodings[c] + encodings[d]\n//     }\n\n//      Deal with the remaining bytes and padding\n//     if (byteRemainder == 1) {\n//       chunk = bytes[mainLength]\n\n//       a = (chunk & 252) >> 2  252 = (2^6 - 1) << 2\n\n//        Set the 4 least significant bits to zero\n//       b = (chunk & 3)   << 4  3   = 2^2 - 1\n\n//       base64 += encodings[a] + encodings[b] + '=='\n//     } else if (byteRemainder == 2) {\n//       chunk = (bytes[mainLength] << 8) | bytes[mainLength + 1]\n\n//       a = (chunk & 64512) >> 10  64512 = (2^6 - 1) << 10\n//       b = (chunk & 1008)  >>  4  1008  = (2^6 - 1) << 4\n\n//        Set the 2 least significant bits to zero\n//       c = (chunk & 15)    <<  2  15    = 2^4 - 1\n\n//       base64 += encodings[a] + encodings[b] + encodings[c] + '='\n//     }\n\n//     return base64\n//   }\n","import { h, FunctionalComponent } from \"preact\";\nimport { useEffect, useState } from \"preact/hooks\";\nimport { useMetaframe, useHashParamBoolean } from \"@metapages/metaframe-hook\";\nimport objectHash from \"object-hash\";\nimport * as tf from \"@tensorflow/tfjs\";\nimport { ButtonClearCache } from \"../../components/ButtonClearCache\";\nimport { TrainingDataPoint, TrainingDataSet } from \"../../lib/metaframe\";\nimport { TrainingData } from \"../../lib/TrainingData\";\nimport { Trainer } from \"../../lib/Trainer\";\nimport { useStore, MessagePayload } from \"../../store\";\nimport { modelToJson } from \"../../lib/io\";\nimport {\n  PersistedModel,\n  PersistedModelJson,\n  PersistedModelMetadata,\n} from \"../../lib/types\";\nimport localForage from \"localforage\";\n\nconst id = \"test\";\n\nconst DEBUG_CACHE_TRAINING_DATA = false;\n\nexport const TabMetaframeTraining: FunctionalComponent = () => {\n  const metaframeObject = useMetaframe();\n  const [nocache] = useHashParamBoolean(\"nocache\");\n  const [trainingDataSet, setTrainingDataSet] = useState<\n    TrainingDataSet | undefined\n  >(undefined);\n\n  const [currentlyTrainingDataHash, setCurrentlyTrainingDataHash] =\n    useState<string>(\"\");\n  const setModel = useStore((state) => state.setModel);\n  const model = useStore((state) => state.model);\n\n  // DEBUGGING load test data set if available\n  useEffect(() => {\n    if (!DEBUG_CACHE_TRAINING_DATA) {\n      return;\n    }\n    (async () => {\n      const trainingDataSet: TrainingDataSet | null = await localForage.getItem(\n        `TrainingDataSet${id}`\n      );\n      if (trainingDataSet) {\n        setTrainingDataSet(trainingDataSet);\n      }\n    })();\n  }, [setTrainingDataSet]);\n\n  // update the trainingDataSet if a new (hashed to check) arrives\n  useEffect(() => {\n    const incomingTrainingData = metaframeObject?.inputs[\"training\"];\n    if (incomingTrainingData && incomingTrainingData !== setTrainingDataSet) {\n      const incomingHash = objectHash(incomingTrainingData);\n      if (incomingHash !== currentlyTrainingDataHash) {\n        setCurrentlyTrainingDataHash(incomingHash);\n        setTrainingDataSet(incomingTrainingData);\n      }\n\n      // Degbugging\n      if (DEBUG_CACHE_TRAINING_DATA) {\n        localForage.setItem(`TrainingDataSet${id}`, incomingTrainingData);\n      }\n    }\n  }, [\n    metaframeObject.inputs,\n    currentlyTrainingDataHash,\n    setCurrentlyTrainingDataHash,\n    setTrainingDataSet,\n  ]);\n\n  const updateModels = useStore((state) => state.updateModels);\n  const setMessages = useStore((state) => state.setMessages);\n  // triggered on a new trainingDataSet: train a new model\n  useEffect(() => {\n    if (!trainingDataSet || !trainingDataSet.examples || trainingDataSet.examples.length === 0 || !metaframeObject?.setOutputs) {\n      return;\n    }\n\n    const messageHeader: MessagePayload = {\n      message: \"Training\",\n      type: \"info\",\n    };\n\n    const keys = Object.keys(trainingDataSet.examples.reduce<Record<string, boolean>>((map, current) => {\n      map[current.label] = true;\n      return map;\n    }, {}));\n\n    if (keys.length < 2) {\n      const messageNotEnoughLabels: MessagePayload = {\n        message: `Not enough data labels: [${keys.join(\", \")}]`,\n        type: \"warning\",\n      };\n      setMessages([messageHeader, messageNotEnoughLabels]);\n      return;\n    }\n\n    let cancelled = false;\n\n    (async () => {\n\n      const messageLoading: MessagePayload = {\n        message: \"loading data...\",\n        type: \"info\",\n      };\n      setMessages([messageHeader, messageLoading]);\n\n      // ? shut down any existing objects to free memory/resources, or they just get GC'd\n      const trainingData = new TrainingData(trainingDataSet);\n      await trainingData.load();\n      if (cancelled) {\n        return;\n      }\n      const messageLoaded: MessagePayload = {\n        message: \"✅ loaded data\",\n        type: \"info\",\n      };\n      setMessages([messageHeader, messageLoaded]);\n\n      const hash = trainingData.hash();\n\n      const meta: PersistedModelMetadata = {\n        prediction: {\n          classNames: trainingData.classNames,\n          imageHeight: trainingData.imageHeight,\n          imageWidth: trainingData.imageWidth,\n          // maxAbsoluteRawValue: trainingData._maxAbsoluteValue,\n        },\n        training: {\n          date: new Date(),\n          hash,\n          ranges: trainingData.ranges,\n        },\n      };\n\n      let model: PersistedModel;\n      // check if we have a cached tensorflow model saved locally\n      // const loadedModel = await tf.loadModel('indexeddb://my-model-1');\n      if (!nocache) {\n        const allModels = await tf.io.listModels();\n        if (cancelled) {\n          return;\n        }\n        if (allModels[`indexeddb://${hash}`]) {\n          const loadedModel = await tf.loadLayersModel(`indexeddb://${hash}`);\n          if (cancelled) {\n            return;\n          }\n\n          model = {\n            model: loadedModel,\n            meta: meta,\n          };\n\n          setMessages([\n            messageHeader,\n            { message: `Model ready (cached) ${hash.substr(0, 10)}`, type: \"success\" },\n          ]);\n          setModel(model);\n          return;\n        }\n      }\n\n      setMessages([messageHeader, { message: \"training...\", type: \"warning\" }]);\n      const trainer = new Trainer(trainingData);\n      await trainer.train();\n      if (cancelled) {\n        return;\n      }\n      setMessages([messageHeader, { message: \"✅ Trained\", type: \"warning\" }]);\n\n      model = {\n        model: trainer.model,\n        meta: meta,\n      };\n\n      if (!nocache) {\n        await model.model.save(`indexeddb://${hash}`);\n        updateModels();\n      }\n\n      setModel(model);\n      const persistedModelJson = await modelToJson(model);\n      metaframeObject.setOutputs!({ model: persistedModelJson });\n\n      setMessages([\n        messageHeader,\n        { message: `✅ Model ready ${hash.substr(0, 10)}`, type: \"success\" },\n      ]);\n    })();\n\n    return () => {\n      cancelled = true;\n    };\n  }, [trainingDataSet, nocache, setModel, metaframeObject?.setOutputs]);\n\n  useEffect(() => {\n    if (model && metaframeObject?.setOutputs) {\n      (async () => {\n        const modelDehydrated:PersistedModelJson = await modelToJson(model);\n        metaframeObject.setOutputs!({model:modelDehydrated});\n      })();\n    }\n  }, [model, metaframeObject?.setOutputs])\n\n  /* id=\"Training\" is consumed by Trainer */\n  return (\n    <div>\n      <ButtonClearCache />\n      <div id=\"Training\" />\n    </div>\n  );\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport { useEffect, useState } from \"preact/hooks\";\nimport { useMetaframe } from \"@metapages/metaframe-hook\";\nimport {\n  predictionDecode,\n  PredictionInput,\n  PredictionInputEncoded,\n  PredictionResult,\n} from \"../../lib/metaframe\";\nimport { useStore } from \"../../store\";\nimport { predict } from \"../../lib/io\";\nimport {\n  Alert,\n  AlertIcon,\n  Table,\n  Tbody,\n  Td,\n  Th,\n  Thead,\n  Tr,\n} from \"@chakra-ui/react\";\n\nexport const TabMetaframePrediction: FunctionalComponent = () => {\n  const metaframeObject = useMetaframe();\n  const model = useStore((state) => state.model);\n  const [predictionDataEncoded, setPredictionDataEncoded] = useState<\n    PredictionInputEncoded | undefined\n  >(undefined);\n  const [predictionInput, setPredictionInput] = useState<\n    PredictionInput | undefined\n  >(undefined);\n  const [prediction, setPrediction] = useState<PredictionResult | undefined>(\n    undefined\n  );\n\n  useEffect(() => {\n    console.log('metaframeObject.inputs', metaframeObject.inputs);\n  }, [metaframeObject.inputs])\n\n  // console.log(\n  //   `TabMetaframePrediction\\n model=${\n  //     model !== undefined\n  //   }\\n predictionDataEncoded=${\n  //     predictionDataEncoded !== undefined\n  //   }\\n predictionInput=${\n  //     predictionInput !== undefined\n  //   }\\n prediction=${prediction}`\n  // );\n\n  // update the predictionInput on the incoming \"prediction\" metaframe input pipe\n  useEffect(() => {\n    const newPredictionData: PredictionInputEncoded =\n      metaframeObject?.inputs?.[\"prediction\"];\n    if (newPredictionData && newPredictionData !== predictionDataEncoded) {\n      setPredictionDataEncoded(newPredictionData);\n      const newPredictionInput: PredictionInput =\n        predictionDecode(newPredictionData);\n      setPredictionInput(newPredictionInput);\n    }\n  }, [\n    metaframeObject.inputs,\n    predictionDataEncoded,\n    setPredictionDataEncoded,\n    setPredictionInput,\n  ]);\n\n  // triggered on a new trainingDataSet: train a new model\n  useEffect(() => {\n    if (!predictionInput || !model || !metaframeObject?.setOutputs) {\n      return;\n    }\n\n    (async () => {\n      const [predictionResult, predictionError] = await predict(\n        model,\n        predictionInput\n      );\n\n      metaframeObject.setOutputs!({\n        prediction: predictionResult,\n        error: predictionError?.message,\n      });\n      setPrediction(predictionResult);\n    })();\n  }, [predictionInput, model, setPrediction, metaframeObject.setOutputs]); //, metaframeObject.setOutputs\n\n  return (\n    <div>\n      <Alert status={prediction?.prediction ? \"success\" : \"info\"}>\n        <AlertIcon />\n        {prediction ? prediction.prediction : \"...waiting for data\"}\n      </Alert>\n\n      {prediction?.note ? (\n        <Alert status=\"warning\">\n          <AlertIcon />\n          {prediction?.note}\n        </Alert>\n      ) : null}\n\n      <Table variant=\"simple\">\n        <Thead>\n          <Tr>\n            <Th>Label</Th>\n            <Th>Score</Th>\n          </Tr>\n        </Thead>\n        <Tbody>\n          {!prediction\n            ? null\n            : Object.keys(prediction.predictions)\n                .sort((p) => prediction.predictions[p])\n                .map((k) => (\n                  <Tr>\n                    <Td>{k}</Td>\n                    <Td isNumeric>{prediction.predictions[k]}</Td>\n                  </Tr>\n                ))}\n        </Tbody>\n      </Table>\n    </div>\n  );\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport {\n  Alert,\n  AlertDescription,\n  AlertIcon,\n  AlertTitle,\n  ListItem,\n  Stack,\n  UnorderedList,\n} from \"@chakra-ui/react\";\nimport { useStore } from \"../store\";\n\nexport const Messages: FunctionalComponent = () => {\n  const messages = useStore((state) => state.messages);\n  return (\n    <Stack spacing={3}>\n      {messages.map((m, i) => (\n        <Alert key={i} status={m.type ? m.type : \"info\"}>\n          <AlertTitle mr={2}>{m.message}</AlertTitle>\n          <AlertIcon />\n          <AlertDescription>\n            <UnorderedList>\n              {!m.messages\n                ? null\n                : m.messages.map((mItem, j) => (\n                    <ListItem key={j}>{mItem}</ListItem>\n                  ))}\n            </UnorderedList>\n          </AlertDescription>\n        </Alert>\n      ))}\n    </Stack>\n  );\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport { AlignedData, Options } from \"uplot\";\nimport \"uplot/dist/uPlot.min.css\";\nimport UplotReact from \"uplot-react\";\nimport { SensorJson } from \"../../lib/TrainingData\";\n\nexport const DataExamplePlot: FunctionalComponent<{ example: SensorJson }> = ({\n  example,\n}) => {\n  const keys = Object.keys(example.data);\n\n  const data: AlignedData = [\n    Array.from(Array(example.data[keys[0]].length).keys()),\n    ...keys.map((k) => Array.from((example.data as any)[k]) as number[]),\n  ];\n\n  const options: Options = {\n    width: 400,\n    height: 100,\n    // class: \"spark\",\n    pxAlign: false,\n    cursor: {\n      show: false,\n    },\n    legend: {\n      show: false,\n    },\n    scales: {\n      x: {\n        time: false,\n      },\n    },\n    axes: [\n      {\n        show: false,\n      },\n      {\n        show: false,\n      },\n    ],\n    series: [\n      {},\n      {\n        stroke: \"#FF6D6D\",\n      },\n      {\n        stroke: \"#FFA56D\",\n      },\n      {\n        stroke: \"#D5B93B\",\n      },\n      {\n        stroke: \"#8089FF\",\n      },\n      {\n        stroke: \"#AA80FF\",\n      },\n      {\n        stroke: \"#80BFFF\",\n      },\n    ],\n  };\n\n  return (\n    <div>\n      <UplotReact options={options} data={data} />\n    </div>\n  );\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport { useEffect, useState } from \"preact/hooks\";\nimport { useStore } from \"../../store\";\nimport { TrainingData } from \"../../lib/TrainingData\";\nimport { DataExamplePlot } from \"./DataExamplePlot\";\nimport {\n  Alert,\n  AlertIcon,\n  Heading,\n  Table,\n  Tabs,\n  Tab,\n  TabList,\n  TabPanel,\n  TabPanels,\n  TabProps,\n  Td,\n  Thead,\n  Tr,\n  Tbody,\n  Tfoot,\n  Th,\n  TableCaption,\n} from \"@chakra-ui/react\";\n\nexport const TabVisualizeTrainingData: FunctionalComponent = () => {\n  const trainingDataSet = useStore((state) => state.trainingDataSet);\n  const [trainingData, setTrainingData] = useState<TrainingData | undefined>();\n\n  // load\n  useEffect(() => {\n    if (!trainingDataSet) {\n      return;\n    }\n    let cancelled = false;\n    (async () => {\n      const trainingDataNew = new TrainingData(trainingDataSet);\n      await trainingDataNew.load();\n      if (cancelled) {\n        return;\n      }\n      setTrainingData(trainingDataNew);\n    })();\n    return () => {\n      cancelled = true;\n    };\n  }, [trainingDataSet]);\n\n  // now plot loaded data\n  useEffect(() => {\n    // trainingData\n  }, [trainingData]);\n\n  return (\n    <Tabs isLazy={true} orientation={\"horizontal\"}>\n      <TabList>\n        {trainingData\n          ? trainingData._labels.map((label, index) => (\n              <Tab key={index}>{label}</Tab>\n            ))\n          : []}\n      </TabList>\n      <TabPanels>\n        {trainingData\n          ? trainingData._labels.map((label, index) => (\n              <TabPanel p={4} key={index}>\n                <TabPane label={label} trainingData={trainingData} />\n              </TabPanel>\n            ))\n          : []}\n      </TabPanels>\n    </Tabs>\n  );\n};\n\nexport const TabPane: FunctionalComponent<{\n  label: string;\n  trainingData: TrainingData;\n}> = ({ label, trainingData }) => {\n  const examples = trainingData.allExamplesForLabel(label);\n  const streamLabels = examples[0] ? Object.keys(examples[0].data).sort() : [];\n\n  return (\n    <div>\n      <Heading size=\"small\">Total: {examples.length}</Heading>\n      <Table variant=\"simple\">\n        {/* <Thead>\n          <Tr>\n            <Th>Name</Th>\n            <Th>Name</Th>\n            <Th>Created</Th>\n            <Th>Delete</Th>\n            <Th>Plot</Th>\n          </Tr>\n        </Thead> */}\n        <Tbody>\n          {examples.map((example, index) => (\n            <Tr key={index}>\n              <Td>{example.url}</Td>\n              <Td>\n                <DataExamplePlot example={example} />\n              </Td>\n            </Tr>\n          ))}\n        </Tbody>\n      </Table>\n    </div>\n  );\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport {\n  Box,\n  Flex,\n  Heading,\n  Link,\n  SimpleGrid,\n  Spacer,\n  Tab,\n  Tabs,\n  TabList,\n  TabPanel,\n  TabPanels,\n  useColorModeValue,\n} from \"@chakra-ui/react\";\nimport { ExternalLinkIcon } from \"@chakra-ui/icons\";\nimport { ButtonOptions } from \"../../components/ButtonOptions\";\nimport { ButtonHelp } from \"../../components/ButtonHelp\";\nimport { TabMetaframeTraining } from \"./TabMetaframeTraining\";\nimport { TabMetaframePrediction } from \"./TabMetaframePrediction\";\nimport { useHashParamInt } from \"@metapages/metaframe-hook\";\nimport { Messages } from \"../../components/Messages\";\nimport { TabVisualizeTrainingData } from './TabVisualizeTrainingData';\n\nexport const TensorFlowRoute: FunctionalComponent = () => {\n  const colors = useColorModeValue(\n    [\"red.50\", \"teal.50\", \"blue.50\"],\n    [\"red.900\", \"teal.900\", \"blue.900\"]\n  );\n  const [index, setIndex] = useHashParamInt(\"tab\", 0);\n  const bg = colors[index!];\n\n  return (\n    <SimpleGrid columns={1} spacing={10}>\n      <Flex>\n        <Heading size=\"md\">\n          <Link href=\"https://www.tensorflow.org/js\" isExternal mr=\"2\">\n            <ExternalLinkIcon /> Tensorflow 1D\n          </Link>\n          convolutional neural net trainer/predictor\n        </Heading>\n        <Spacer />\n        <ButtonHelp />\n        <ButtonOptions />\n      </Flex>\n      <Flex>\n        <Messages />\n      </Flex>\n      <Box>\n        <Tabs isLazy={true} isFitted={true} onChange={setIndex} bg={bg}>\n          <TabList>\n            <Tab>Visualize training</Tab>\n            <Tab>Train</Tab>\n            <Tab>Predict</Tab>\n          </TabList>\n          <TabPanels p=\"1rem\">\n            <TabPanel>\n              <TabVisualizeTrainingData />\n            </TabPanel>\n            <TabPanel>\n              <TabMetaframeTraining />\n            </TabPanel>\n            <TabPanel>\n              <TabMetaframePrediction />\n            </TabPanel>\n          </TabPanels>\n        </Tabs>\n      </Box>\n    </SimpleGrid>\n  );\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport { TensorFlowRoute } from \"./tensorflow\";\n\nexport const Home: FunctionalComponent = () => {\n  return (\n    <div>\n      <TensorFlowRoute />\n    </div>\n  );\n};\n","import { h, FunctionalComponent } from \"preact\";\nimport { useEffect } from \"preact/hooks\";\nimport { useMetaframe } from \"@metapages/metaframe-hook\";\nimport { Home } from \"./routes/home\";\nimport { PredictionInputEncoded, TrainingDataSet } from \"./lib/metaframe\";\nimport { useStore, MessagePayload } from \"./store\";\n\nexport const App: FunctionalComponent = () => {\n\n  // Wire up the metaframe to the store\n  const metaframeObject = useMetaframe();\n  const setTrainingDataSet = useStore((state) => state.setTrainingDataSet);\n  const setPredictionInput = useStore((state) => state.setPredictionInput);\n  useEffect(() => {\n    if (!metaframeObject.metaframe) {\n      return;\n    }\n    const disposers: (()=>void)[] = [];\n    disposers.push(metaframeObject.metaframe.onInput(\"training\", (training : TrainingDataSet) => {\n      if (training) {\n        setTrainingDataSet(training);\n      }\n    }));\n\n    disposers.push(metaframeObject.metaframe.onInput(\"prediction\", (predictionInput : PredictionInputEncoded) => {\n      if (predictionInput) {\n        setPredictionInput(predictionInput);\n      }\n    }));\n\n    return () => {\n      while(disposers.length > 0) disposers.pop()();\n    }\n  }, [metaframeObject.metaframe, setTrainingDataSet]);\n  \n  return <Home />;\n};\n","import { h, render } from \"preact\";\nimport { WithMetaframe } from \"@metapages/metaframe-hook\";\nimport { ChakraProvider } from \"@chakra-ui/react\";\nimport { App } from \"./App\";\nimport localForage from \"localforage\";\n\nlocalForage.config({\n  driver: localForage.INDEXEDDB,\n  name: \"metaframe-predictor\",\n  version: 1.0,\n  storeName: \"models\", // Should be alphanumeric, with underscores.\n  description: \"Stores tensorflow models locally\",\n});\n\nrender(\n  <ChakraProvider>\n    <WithMetaframe>\n      <App />\n    </WithMetaframe>\n  </ChakraProvider>,\n  document.getElementById(\"root\")!\n);\n"],"names":["useState","useCallback","Fragment","DrawerOverlay","DrawerCloseButton","DrawerBody","tf.io","tf.tensor2d","tf.util","tf.sequential","tf.layers","tf.train","tfvis.show","tf.tidy","tf.tensor3d","tf.loadLayersModel","render"],"mappings":"igBAGA,KAAM,IAAuB,CAC3B,CACE,KAAM,UACN,YAAa,kBACb,KAAM,UACN,QAAS,IAEX,CACE,KAAM,iBACN,YAAa,sBACb,KAAM,UACN,QAAS,KAUA,GAAqC,MACxC,GAAD,CAAmB,QAAS,KCXxB,GAAoD,CAAC,CAAE,SAAU,MAEtE,CAAC,GAAuB,GAA6B,WACrD,EAAyB,GAC7B,EAAoB,eAEhB,CAAC,EAAM,GAAWA,EAAkB,KAAa,GAAQ,CAAC,GAE1D,EAAUC,EAAY,IAAM,GACxB,CAAC,IACR,CAAC,aAEE,GAEF,GAAG,OAAO,SAAS,SAAS,OAAO,SAAS,sBAG7CC,GAAD,OACG,GAAD,CACE,cAAc,MACd,aAAW,OAEX,OAAO,GAAD,MACN,KAAK,KACL,UACA,GAAG,QAEJ,GAAD,CAAW,MAAU,OAAQ,EAAM,cAKnC,GAID,CAAC,CAAE,SAAQ,UAAS,SAAU,MAC3B,GAAUD,EAAY,IAAM,GACxB,CAAC,IACR,CAAC,EAAS,IAEP,EAAiBA,EAAY,IAAM,GAC/B,KACP,CAAC,IAEE,EAAY,wDAAwD,aAGvE,GAAD,CACE,KAAK,OACL,UAAU,MACV,UACA,SACA,oBAECE,GAAD,OACG,GAAD,OACGC,GAAD,CAAmB,KAAK,KAAK,YAAY,WAAW,GAAG,eACtDC,GAAD,OACG,SAAD,CAAQ,MAAM,OAAO,OAAO,OAAO,IAAK,SCvCvC,EAAW,GAAmB,KACzC,SAAU,GACV,WAAY,GACZ,MAAO,OACP,gBAAiB,OACjB,gBAAiB,OACjB,iBAAkB,OAClB,0BAA2B,KAC3B,cAAe,IAAM,EAAI,AAAC,KAAa,SAAU,MACjD,WAAY,AAAC,GACX,EAAI,AAAC,KAAa,SAAU,EAAM,SAAS,OAAO,CAAC,OACrD,YAAa,AAAC,GAA+B,EAAI,AAAC,KAAa,cAC/D,mBAAoB,AAAC,GACnB,EAAI,AAAC,KAAa,qBAEpB,aAAc,SAAY,MAClB,GAAQ,KAAM,QAChB,AAAC,KAAa,WAAY,MAEhC,aAAc,SAAY,MAClB,QACF,AAAC,KAAa,WAAY,MAEhC,SAAU,KAAO,IAA0B,GACrC,AAAC,KAAa,YAEpB,mBAAoB,KAAO,IAA4C,GACjE,AAAC,KAAa,sBAEpB,oBAAqB,KAAO,IAAuC,GAC7D,AAAC,KAAa,0BAIhB,GAAgB,SAAY,MAC1B,GAAY,KAAMC,GAAM,mBACvB,QAAO,KAAK,GAAW,QAG1B,GAAkB,SAAY,MAC5B,GAAY,KAAMA,GAAM,aACxB,EAAY,OAAO,KAAK,GAAW,IAAI,AAAC,GAAQA,EAAM,YAAY,OACpE,MACI,SAAQ,IAAI,SACX,WACC,MAAM,KC1EL,GAAwC,IAAM,MACnD,GAAa,EAAS,AAAC,GAAU,EAAM,YACvC,EAAe,EAAS,AAAC,GAAU,EAAM,cACzC,EAAkB,EAAS,AAAC,GAAU,EAAM,cAE5C,EAAUL,EAAY,SAAY,MAErC,CAAC,aAGM,IAAM,CACb,UAAY,MACL,UAEP,CAAC,MAGD,GAAD,CAAQ,WAAY,GAAc,EAAG,WAAkB,gBACvC,EAAa,EAAI,EAAa,EAAE,aCEvC,EAAkE,GAAU,MACjF,GAAuB,iBACtB,KAAK,GAAQ,QAAQ,GAAK,CAC3B,EAAO,OACF,GAAK,GAAI,cAAa,EAAa,EAAO,QAG9C,GAGI,GAA2E,YAC9E,IAAI,mBAAoB,GACA,CAC9B,UAAW,EAAW,UACtB,OAAQ,EAAmB,EAAW,UAgOpC,EAAQ,mEAGR,EAAS,GAAI,YAAW,KAC9B,OAAS,GAAI,EAAG,EAAI,EAAM,OAAQ,MACzB,EAAM,WAAW,IAAM,aAGH,EAAmC,IAC1D,GAAQ,GAAI,YAAW,GACvB,EACA,EAAM,EAAM,OACZ,EAAS,OAER,EAAI,EAAG,EAAI,EAAK,GAAK,KACd,EAAM,EAAM,IAAM,MAClB,KAAc,GAAK,IAAM,EAAM,EAAM,EAAI,IAAM,MAC/C,KAAc,EAAI,GAAK,KAAO,EAAM,EAAM,EAAI,IAAM,MACpD,EAAM,EAAM,EAAI,GAAK,UAG7B,GAAM,GAAM,IACL,EAAO,UAAU,EAAG,EAAO,OAAS,GAAK,IACzC,EAAM,GAAM,MACZ,EAAO,UAAU,EAAG,EAAO,OAAS,GAAK,MAG7C,aAGoB,EAA8B,IACnD,CAAC,OACK,IAAI,OAAM,yCAElB,GAAe,EAAO,OAAS,IACjC,EAAM,EAAO,OACb,EACA,EAAI,EACJ,EACA,EACA,EACA,EAEE,EAAO,EAAO,OAAS,KAAO,UAE5B,EAAO,EAAO,OAAS,KAAO,aAKhC,GAAc,GAAI,aAAY,GAChC,EAAQ,GAAI,YAAW,OAEpB,EAAI,EAAG,EAAI,EAAK,GAAK,IACb,EAAO,EAAO,WAAW,MACzB,EAAO,EAAO,WAAW,EAAI,MAC7B,EAAO,EAAO,WAAW,EAAI,MAC7B,EAAO,EAAO,WAAW,EAAI,MAElC,KAAQ,GAAY,EAAM,GAAY,IACtC,QAAoB,KAAO,EAAM,GAAY,IAC7C,QAAoB,IAAM,EAAM,EAAW,SAG5C,GCvUT;AAAA;AAAA;AAAA;AAAA,GA0CA,KAAM,IAA4B,UASR,CA6BxB,YAAY,EAA+B,WAxBD,eAG0B,gBAG/C,iBAEE,2BAGO,yBACD,uBAEG,GAAI,iCACL,GAAI,kCAEF,GAAI,oCACL,GAAI,4BAEd,OAMf,iBAAmB,EAG1B,MAAe,IACT,CAAC,KAAK,sBACF,oCAED,GAAW,KAAK,kBAGzB,eAAe,EAAmB,OACzB,MAAK,UAAU,EAAW,KAAK,iBAGxC,cAAc,EAAmB,OACxB,MAAK,UAAU,EAAW,KAAK,gBAGxC,UAAU,EAAmB,EAAsB,MAC3C,GAAmB,GAAI,cAAa,EAAY,KAAK,WACrD,EAAmB,GAAI,YAAW,EAAY,KAAK,oBACjD,OAAO,KAAK,WAAa,UAExB,GAAI,EAAG,EAAI,EAAW,IAAK,MAE5B,GAAM,EAAQ,EAAI,EAAQ,QAE1B,EAAiB,EAAM,KAAK,UAC5B,EACF,KAAK,gBAAgB,MAAM,EAAgB,EAAiB,KAAK,aACpD,IAAI,EAAO,EAAI,KAAK,gBAE/B,GAAmB,EAAM,KAAK,WAC9B,EACJ,KAAK,iBAAiB,MAAM,EAAkB,EAAmB,KAAK,cACvD,IAAI,EAAO,EAAI,KAAK,iBAGjC,GAAKM,EAAY,EAAkB,CAAC,EAAW,KAAK,YACpD,EAASA,EAAY,EAAkB,CAAC,EAAW,KAAK,mBAWvD,CAAC,KAAI,aA+IV,aAAa,OACR,MAAK,WAGV,eAAe,OACV,MAAK,cAGV,aAAa,OACR,MAAK,QAAQ,UAGlB,aAAa,OACR,MAAK,SAAS,UAGnB,QAAQ,OACH,MAAK,cAGV,cAAc,OACT,MAAK,aAGV,SAAS,OACJ,MAAK,eAGV,YAAY,OACP,MAAK,WAAa,KAAK,eAG5B,YAAY,OACP,MAAK,cAGV,uBAAuB,OAClB,MAAK,gBAAgB,UAG1B,sBAAsB,OACjB,MAAK,eAAe,OAsM7B,iBAAiB,EAA4I,MACtJ,wBAAwB,CAAC,EAA2B,EAAmB,EAAqB,IAAuB,GACxG,QAAQ,CAAC,EAAyB,IAAiC,GAC/D,EAAiB,EAAsB,EAAa,EAAY,EAAc,OAapG,iBAAiB,EAAqG,QAC7G,KAAK,KAAK,MAAM,QAAQ,AAAC,GAAwB,CACrC,KAAK,KAAK,GAClB,QAAQ,CAAC,EAAa,IAAiB,MACxC,GAAU,EAAY,UACvB,SAAS,QAAQ,AAAC,GAAuB,GAChC,EAAQ,GAAa,EAAY,EAAc,SAMnE,wBAAwB,EAAqG,QACpH,KAAK,KAAK,MAAM,QAAQ,AAAC,GAAwB,CACrC,KAAK,KAAK,GAClB,QAAQ,CAAC,EAAa,IAAiB,MACxC,GAAU,EAAY,UACvB,SAAS,QAAQ,AAAC,GAAuB,CACtC,EAAW,SAAS,QAGd,EAAQ,GAAa,EAAY,EAAc,SAMnE,YAAY,EAAkF,QACrF,KAAK,KAAK,MAAM,QAAQ,AAAC,GAAwB,CACrC,KAAK,KAAK,GAClB,QAAQ,CAAC,EAAa,IAAiB,MACxC,GAAU,EAAY,OAChB,EAAS,EAAc,OAKzC,oBAAoB,EAAe,OAC1B,MAAK,KAAK,GAanB,aAAc,IACR,GAAM,OACJ,GAAO,AAAC,GAAgB,GACtB,KAAK,IAAI,KAAK,IAAI,GAAM,gBAE3B,iBAAiB,GACf,EAST,WAAY,SACF,IAAI,2BAEN,GAAuB,KAAK,SAAS,OAAO,GAAK,CAAC,EAAE,SAAS,QAE9C,QAAQ,GAAU,KAAK,OAAO,GAAU,CAAC,IAAI,OAAO,UAAW,IAAI,OAAO,UAAW,OAAO,OAAO,iBACnH,wBAAwB,CAAC,EAA2B,EAAmB,EAAqB,IAAuB,GACxG,QAAQ,AAAC,GAA4B,MAC1C,OAAO,GAAY,IAAM,KAAK,IAAI,KAAK,OAAO,GAAY,IAAK,QAC/D,OAAO,GAAY,IAAM,KAAK,IAAI,KAAK,OAAO,GAAY,IAAK,SAKrD,QAAQ,GAAU,KAAK,OAAO,GAAQ,OAAS,KAAK,IAAI,KAAK,OAAO,GAAQ,IAAK,KAAK,IAAI,KAAK,OAAO,GAAQ,YAE9H,wBAAwB,CAAC,EAA2B,EAAmB,EAAqB,IAAuB,MAChH,GAAY,KAAK,OAAO,GAAY,SAC9B,QAAQ,CAAC,EAAW,IAAiB,GACnC,GAAS,EAAY,GAAS,cAOtC,IAAI,uBAAuB,KAAK,UAAU,KAAK,WAUzD,UAAW,SACD,IAAI,4BAEP,YAAY,AAAC,GAAY,MACpB,GAAc,KAAK,SAAS,OAAO,GAAK,EAAE,SAAS,SAGrD,GAAkB,OAAO,YACjB,QAAQ,GAAc,GACxB,GAAY,QAAQ,CAAC,EAAwB,IAAM,EAAU,KAAK,IAAI,EAAS,QAI7E,QAAQ,GAAc,GACtB,GAAY,QAAQ,CAAC,EAAc,IAAkB,GACjD,GAAY,GAAS,EAAO,QA4BlD,6BAA8B,SACpB,IAAI,iDACR,GAAM,EACN,EAAS,OACP,GAAqB,KAAK,SAAS,OAAO,GAAK,CAAC,EAAE,SAAS,WAC5D,YAAY,CAAC,EAAS,EAAG,IAAY,IAEpC,CAAC,KAAK,iBAAiB,eAAiB,CAAC,KAAK,iBAAiB,cAAc,SAAS,GAAU,IAI9F,CAAC,EAAQ,KAAK,SAAS,cAGvB,GAAc,EAAQ,KAAK,SAAS,IAAI,OAAS,OAC7C,GAAe,GAEf,IAAmB,OAAO,AAAC,GAAW,EAAQ,GAAQ,IAAgB,GAAG,OAAS,GAFjE,OAMnB,KAAK,IAAI,EAAK,EAAc,KAE3B,KAAK,IAAI,EAAQ,EAAQ,KAAK,SAAS,IAAI,kBAE9C,IAAI,gBAAgB,cAAgB,yBACvC,YAAY,CAAC,EAAS,EAAG,IAAY,MACjC,SAAS,QAAQ,AAAC,GAAW,CAC1B,CAAC,EAAQ,OAGL,GAAU,EAAQ,GAAQ,MAAM,EAAG,UAW1C,OACJ,iBAAiB,AAAC,GAAgB,GAC5B,KAAK,IAAI,EAAQ,EAAY,kBAEhC,IAAI,+BAA+B,MAK7C,QAAS,SACC,IAAI,uBACR,GAAe,OACb,GAAuB,QAGxB,iBAAiB,AAAC,GAAgB,GACtB,KAAK,IAAI,EAAc,EAAY,UACtC,QAAQ,CAAC,EAAe,IAAiB,CAC/C,GAAS,KAGF,KAAK,EAAQ,EAAY,EAAQ,aAqB3C,WAAa,UACV,IAAI,0BAA0B,KAAK,aAgB7C,kBAAkB,EAAqC,IACjD,CAAC,OACG,qCAEJ,KAAK,aAAe,OAChB,qDAOD,KAAK,GAAS,QAAQ,GAAU,IACjC,EAAQ,GAAQ,OAAS,KAAK,eACxB,GAAU,EAAQ,GAAQ,MAAM,EAAG,KAAK,aAG9C,EAAQ,GAAQ,OAAS,KAAK,WAAY,MACtC,GAAO,GAAI,cAAa,KAAK,cAC9B,IAAI,EAAQ,MACT,GAAU,YAKf,KAAK,GAAS,QAAQ,AAAC,GAAmB,MACzC,GAAY,KAAK,OAAO,GAAQ,SAC9B,GAAQ,QAAQ,CAAC,EAAa,EAAc,IAAQ,GACtD,GAAS,EAAM,WAIjB,GAAY,GAAI,cAAa,EAAI,KAAK,cAExC,GAAmB,cAElB,SAAS,QAAQ,AAAC,GAAW,MAC1B,GAAS,EAAmB,KAAK,YAC7B,IAAI,EAAQ,GAAS,SAK1B,OAGH,OAAO,SACH,IAAI,+BAA+B,KAAK,iBAAiB,SAAS,0DACrE,KAAO,QAEN,GAAO,KAAK,UACb,SAAW,QACV,GAAiC,WAC/B,IAAI,MAAM,kBAAkB,SAAS,SACxC,iBAAiB,SAAS,QAAQ,AAAC,GAA+B,MAC7D,GAAQ,EAAQ,MACjB,EAAK,OACD,GAAS,SAEZ,GAAyB,EAAmB,EAAQ,KAAK,eACxD,KAAK,GAAW,QAAQ,GAAK,EAAQ,GAAK,SAC3C,GAA+B,CAAC,KAAK,EAAU,IAAK,EAAQ,MAAQ,EAAQ,OAC7E,GAAO,KAAK,UAGhB,SAAW,OAAO,KAAK,QACvB,SAAS,eACN,IAAI,gBAAiB,KAAK,eAC7B,QAAU,OAAO,KAAK,KAAK,cACxB,IAAI,eAAgB,KAAK,cAC5B,QAAQ,eACL,IAAI,cAAc,KAAK,QAAQ,KAAK,uBACpC,IAAI,kEACP,mCACA,iBACC,GAAgB,KAAK,yBACnB,IAAI,yBAAyB,KACjC,GAAiB,OACb,2BAEH,gBACA,iBAGG,IAAI,yDAEN,GAAoB,OAAO,KAAK,KAAK,MAAM,OAAO,CAAC,EAAK,IAAU,EAAM,KAAK,KAAK,GAAO,OAAQ,QAClG,gBAAkB,GAAI,cAAa,EAAoB,KAAK,gBAC5D,iBAAmB,GAAI,YAAW,EAAoB,KAAK,oBAExD,IAAI,6BAA6B,eAA+B,KAAK,wBAAwB,KAAK,sBAClG,IAAI,qBAAqB,KAAK,gBAAgB,KAAK,aAEvD,GAAa,OACZ,WAAW,QAAQ,CAAC,EAAa,IAAiB,CAEpC,KAAK,KAAK,GAGlB,QAAQ,AAAC,GAAgB,MAE3B,iBAAkB,EAAa,KAAK,WAAc,GAAgB,OAEjE,GAAU,EAAY,QAGxB,GAAmB,OAEhB,SAAS,QAAQ,AAAC,GAAW,MAC1B,GAAU,EAAa,KAAK,UAAc,EAAmB,KAAK,eAEnE,gBAAgB,IAAI,EAAQ,GAAS,sBAgB5C,GAA4BC,GAAQ,sBAAsB,GAC1D,EAAsB,KAAK,MAAM,GAA4B,QAE9D,gBAAkB,EAA0B,MAAM,EAAG,QACrD,eAAiB,EAA0B,MAAM,IC34B1D;AAAA;AAAA;AAAA,GAWA,KAAM,IAAa,GACb,GAAS,WAEM,CAKnB,YAAY,EAAmB,iBAFN,QAGlB,MAAQ,OACR,iBAGH,QAAQ,OACH,MAAK,OAGd,aAAc,MACN,GAAO,KAAK,WACb,WAAa,EAAK,gBAIlB,OAASC,UA0BT,OAAO,IAAIC,EAAU,OAAO,CAAE,QAAS,IAAK,WAAY,EAAG,QAAS,EAAG,WAAY,OAAQ,kBAAmB,kBAAmB,WAAY,CAAC,EAAK,OAAQ,EAAK,eAChK,OAAO,IAAIA,EAAU,OAAO,CAAE,QAAS,IAAK,WAAY,EAAG,QAAS,EAAG,WAAY,eACnF,OAAO,IAAIA,EAAU,OAAO,CAAE,QAAS,IAAK,WAAY,EAAG,QAAS,EAAG,WAAY,eACnF,OAAO,IAAIA,EAAU,aAAa,CAAC,SAAS,UAC5C,OAAO,IAAIA,EAAU,OAAO,CAAE,QAAS,IAAK,WAAY,EAAG,QAAS,EAAG,WAAY,eAEnF,OAAO,IAAIA,EAAU,+BACrB,OAAO,IAAIA,EAAU,QAAQ,CAAC,KAAK,QAK9B,UAGV,KAAK,OAAO,IAAIA,EAAU,MAAM,CAAE,MAAO,EAAK,WAAW,OAAQ,WAAY,kBAGxE,OAAO,eAIN,GAAYC,GAAS,mBACtB,OAAO,QAAQ,CAChB,YACA,KAAM,0BACN,QAAS,CAAC,cAGP,KAAK,YAGR,QAAQ,MACN,GAAO,KAAK,MACZ,EAAU,CAAC,OAAQ,WAAY,MAAO,WAItC,EAAY,SAAS,eAAe,YACpC,EAAeC,QAAW,aAAa,EAAW,GAOlD,EAAmB,EAAK,qBAAuB,EAC/C,CAAC,EAAS,GAAWC,EAAQ,IAAM,MAC/B,GAAI,EAAK,eAAe,SACvB,CAGL,EAAE,GAAG,QAAQ,CAAC,EAAkB,EAAK,YAAa,EAAK,aACvD,EAAE,UAOF,EAAkB,EAAK,oBAAsB,EAC7C,CAAC,EAAQ,GAAUA,EAAQ,IAAM,MAC7B,GAAI,EAAK,cAAc,SACtB,CAGL,EAAE,GAAG,QAAQ,CAAC,EAAiB,EAAK,YAAa,EAAK,aACtD,EAAE,UAIF,EAAI,KAAM,MAAK,MAAM,IAAI,EAAS,EAAS,CAC7C,UAAW,GACX,eAAgB,CAAC,EAAQ,GACzB,OAAQ,GACR,QAAS,GACT,UAAW,gBAGV,MAGE,QCrHE,GAAc,KAAO,IAAoD,MAC9E,GAAe,GAAI,IACN,KAAM,GAAE,MAAM,KAAK,QAEhC,GAAyC,OAAO,OAAO,GAAI,YAC9C,MAAQ,EAAa,UACjC,GAGT,QAA+C,CAE7C,YAAY,EAAc,MACnB,UAAY,OAGb,MAAK,EAAoE,MACvE,GAA+B,CACnC,mBAAoB,CAClB,UAAW,GAAI,MACf,kBAAmB,OACnB,mBAAqB,EAAc,cAA8B,WAEjE,gBAAkB,EAAc,WAA2B,gBAG3D,aACG,UAAY,OAAO,OAAO,GAAI,QAC9B,UAAU,cAAgB,EAAa,EAAc,oBACrD,UAAU,WAAa,EAAa,EAAc,YAChD,QACA,YACI,OAAS,CAAC,SACd,QAGL,OAAyC,SACrC,IAAI,8BACN,GAAuC,OAAO,OAAO,GAAI,KAAK,oBACrD,cAAgB,EAAa,KAAK,UAAU,iBAC5C,WAAa,EAAa,KAAK,UAAU,oBAChD,IAAI,+BAAgC,GACrC,QAIE,IAAU,MACrB,EACA,IAC+D,IAC3D,CAAC,QACI,CACL,OACA,GAAI,OAAM,mDAGV,CAAC,QACI,CACL,OACA,GAAI,OAAM,qCAIV,CAAC,EAAM,aACF,CACL,OACA,GAAI,OAAM,yDAIR,GAAkC,EAAM,OACxC,EAAkB,GAAkB,EAAO,GAC3C,EAAKC,GAAY,EAAiB,CACtC,EACA,EAAM,KAAK,WAAW,YACtB,EAAM,KAAK,WAAW,aAElB,EACJ,EAAM,MAAM,eAAe,GAC3B,cAEE,GAAkB,EAClB,EAAgC,GAChC,OACE,GAEF,YAEE,KAAK,WAAW,WAAW,QAAQ,CAAC,EAAe,IAAkB,GAC3D,GAAS,EAAW,GAC9B,EAAW,GAAS,MACZ,EAAW,KACG,KAIxB,IAA0B,KAAO,EAAc,GAAyB,KAAQ,EAAc,MACzF,GAAG,gCACc,KAWnB,CARkC,CACvC,WAAY,EACZ,YAAa,EACb,UAAW,EAAM,UACjB,UAAW,EAAM,KAAK,SAAS,KAC/B,QAGsB,SAUb,GAAoB,CAC/B,EACA,IACiB,IACb,CAAC,OACG,qCAEJ,CAAC,OACG,uCAID,KAAK,GAAS,QAAQ,AAAC,GAAmB,CAC3C,EAAO,SAAS,YACX,GAAQ,UAIb,GAAO,OAAO,KAAK,KACpB,eAEG,IAAI,OAAQ,QAGd,GAAY,KAAK,IACrB,EAAM,KAAK,WAAW,YACtB,EAAM,KAAK,WAAW,cAInB,QAAQ,AAAC,GAAW,IAEnB,IAAU,IAAS,OAAS,MACtB,GAAU,EAAQ,GAAQ,MAAM,EAAG,IAGzC,IAAU,IAAS,OAAS,EAAW,MACnC,GAAO,GAAI,cAAa,KACzB,IAAI,EAAQ,MACT,GAAU,OAKjB,QAAQ,AAAC,GAAmB,GACvB,GAAQ,QACd,CAAC,EAAa,EAAe,IAAmC,GAC1D,GAAS,EAAM,EAAO,KAAK,SAAS,OAAO,GAAQ,gBAOvD,GAAY,GAAI,cACpB,EAAI,EAAM,KAAK,WAAW,WAAa,EAAM,KAAK,WAAW,sBAE1D,QAAQ,CAAC,EAAQ,IAAgB,MAC9B,GAAiB,EAAc,IAC3B,IAAI,EAAQ,GAAS,KAG1B,GCvLI,GAA4C,IAAM,MACvD,GAAkB,IAClB,CAAC,GAAW,GAAoB,WAChC,CAAC,EAAiB,GAAsBd,EAE5C,QAEI,CAAC,EAA2B,GAChCA,EAAiB,IACb,EAAW,EAAS,AAAC,GAAU,EAAM,UACrC,EAAQ,EAAS,AAAC,GAAU,EAAM,SAG9B,IAAM,GAYb,CAAC,MAGM,IAAM,MACR,GAAuB,GAAiB,OAAO,YACjD,GAAwB,IAAyB,EAAoB,MACjE,GAAe,EAAW,GAC5B,IAAiB,MACU,KACV,MAQtB,CACD,EAAgB,OAChB,EACA,EACA,SAGI,GAAe,EAAS,AAAC,GAAU,EAAM,cACzC,EAAc,EAAS,AAAC,GAAU,EAAM,sBAEpC,IAAM,IACV,CAAC,GAAmB,CAAC,EAAgB,UAAY,EAAgB,SAAS,SAAW,GAAK,CAAC,GAAiB,uBAI1G,GAAgC,CACpC,QAAS,WACT,KAAM,QAGF,EAAO,OAAO,KAAK,EAAgB,SAAS,OAAgC,CAAC,EAAK,OAClF,EAAQ,OAAS,GACd,GACN,QAEC,EAAK,OAAS,EAAG,MACb,GAAyC,CAC7C,QAAS,4BAA4B,EAAK,KAAK,SAC/C,KAAM,aAEI,CAAC,EAAe,cAI1B,GAAY,GAEf,gBAAY,GAMC,CAAC,EAJ0B,CACrC,QAAS,kBACT,KAAM,eAKF,GAAe,GAAI,GAAa,WAChC,GAAa,OACf,WAOQ,CAAC,EAJyB,CACpC,QAAS,qBACT,KAAM,eAIF,GAAO,EAAa,OAEpB,EAA+B,CACnC,WAAY,CACV,WAAY,EAAa,WACzB,YAAa,EAAa,YAC1B,WAAY,EAAa,YAG3B,SAAU,CACR,KAAM,GAAI,MACV,OACA,OAAQ,EAAa,YAIrB,MAGA,CAAC,EAAS,MACN,IAAY,KAAMM,GAAM,gBAC1B,YAGA,GAAU,eAAe,KAAS,MAC9B,IAAc,KAAMS,IAAmB,eAAe,QACxD,WAII,CACN,MAAO,GACP,UAGU,CACV,EACA,CAAE,QAAS,wBAAwB,EAAK,OAAO,EAAG,MAAO,KAAM,eAExD,aAKD,CAAC,EAAe,CAAE,QAAS,cAAe,KAAM,kBACtD,GAAU,GAAI,IAAQ,WACtB,GAAQ,QACV,WAGQ,CAAC,EAAe,CAAE,QAAS,iBAAa,KAAM,eAElD,CACN,MAAO,EAAQ,MACf,QAGG,SACG,GAAM,MAAM,KAAK,eAAe,YAI/B,QACH,IAAqB,KAAM,GAAY,KAC7B,WAAY,CAAE,MAAO,OAEzB,CACV,EACA,CAAE,QAAS,sBAAiB,EAAK,OAAO,EAAG,MAAO,KAAM,iBAIrD,IAAM,GACC,KAEb,CAAC,EAAiB,EAAS,EAAU,GAAiB,eAE/C,IAAM,CACV,GAAS,GAAiB,sBACf,MACL,GAAqC,KAAM,GAAY,KAC7C,WAAY,CAAC,MAAM,SAGtC,CAAC,EAAO,GAAiB,eAIzB,MAAD,OACG,GAAD,QACC,MAAD,CAAK,GAAG,eC5LD,GAA8C,IAAM,MACzD,GAAkB,IAClB,EAAQ,EAAS,AAAC,GAAU,EAAM,OAClC,CAAC,EAAuB,GAA4Bf,EAExD,QACI,CAAC,EAAiB,GAAsBA,EAE5C,QACI,CAAC,EAAY,GAAiBA,EAClC,iBAGQ,IAAM,SACN,IAAI,yBAA0B,EAAgB,SACrD,CAAC,EAAgB,WAaV,IAAM,MACR,GACJ,GAAiB,QAAS,cACxB,GAAqB,IAAsB,EAAuB,GAC3C,QACnB,GACJ,GAAiB,KACA,KAEpB,CACD,EAAgB,OAChB,EACA,EACA,MAIQ,IAAM,CACV,CAAC,GAAmB,CAAC,GAAS,CAAC,GAAiB,sBAIvC,MACL,CAAC,EAAkB,GAAmB,KAAM,IAChD,EACA,KAGc,WAAY,CAC1B,WAAY,EACZ,MAAO,GAAiB,YAEZ,QAEf,CAAC,EAAiB,EAAO,EAAe,EAAgB,eAGxD,MAAD,OACG,EAAD,CAAO,OAAQ,GAAY,WAAa,UAAY,UACjD,EAAD,MACC,EAAa,EAAW,WAAa,uBAGvC,GAAY,OACV,EAAD,CAAO,OAAO,aACX,EAAD,MACC,GAAY,MAEb,OAEH,EAAD,CAAO,QAAQ,YACZ,GAAD,OACG,EAAD,OACG,EAAD,KAAI,WACH,EAAD,KAAI,aAGP,EAAD,KACG,AAAC,EAEE,OAAO,KAAK,EAAW,aACpB,KAAK,AAAC,GAAM,EAAW,YAAY,IACnC,IAAI,AAAC,KACH,EAAD,OACG,EAAD,KAAK,KACJ,EAAD,CAAI,UAAS,IAAE,EAAW,YAAY,MAN5C,SCjGD,GAAgC,IAAM,MAC3C,GAAW,EAAS,AAAC,GAAU,EAAM,mBAExC,GAAD,CAAO,QAAS,GACb,EAAS,IAAI,CAAC,EAAG,MACf,EAAD,CAAO,IAAK,EAAG,OAAQ,EAAE,KAAO,EAAE,KAAO,UACtC,GAAD,CAAY,GAAI,GAAI,EAAE,WACrB,EAAD,QACC,GAAD,OACG,GAAD,KACG,AAAC,EAAE,SAEA,EAAE,SAAS,IAAI,CAAC,EAAO,MACpB,GAAD,CAAU,IAAK,GAAI,IAFrB,y0DCjBL,IAAgE,CAAC,CAC5E,aACI,MACE,GAAO,OAAO,KAAK,EAAQ,MAE3B,EAAoB,CACxB,MAAM,KAAK,MAAM,EAAQ,KAAK,EAAK,IAAI,QAAQ,QAC/C,GAAG,EAAK,IAAI,AAAC,GAAM,MAAM,KAAM,EAAQ,KAAa,eAmDnD,MAAD,OACG,GAAD,CAAY,QAjDS,CACvB,MAAO,IACP,OAAQ,IAER,QAAS,GACT,OAAQ,CACN,KAAM,IAER,OAAQ,CACN,KAAM,IAER,OAAQ,CACN,EAAG,CACD,KAAM,KAGV,KAAM,CACJ,CACE,KAAM,IAER,CACE,KAAM,KAGV,OAAQ,CACN,GACA,CACE,OAAQ,WAEV,CACE,OAAQ,WAEV,CACE,OAAQ,WAEV,CACE,OAAQ,WAEV,CACE,OAAQ,WAEV,CACE,OAAQ,aAOoB,WCxCvB,GAAgD,IAAM,MAC3D,GAAkB,EAAS,AAAC,GAAU,EAAM,iBAC5C,CAAC,EAAc,GAAmBA,aAG9B,IAAM,IACV,CAAC,YAGD,GAAY,GACf,gBAAY,MACL,GAAkB,GAAI,GAAa,QACnC,GAAgB,OAClB,MAGY,OAEX,IAAM,GACC,KAEb,CAAC,MAGM,IAAM,GAEb,CAAC,MAGD,EAAD,CAAM,OAAQ,GAAM,YAAa,gBAC9B,EAAD,KACG,EACG,EAAa,QAAQ,IAAI,CAAC,EAAO,MAC9B,EAAD,CAAK,IAAK,GAAQ,IAEpB,MAEL,EAAD,KACG,EACG,EAAa,QAAQ,IAAI,CAAC,EAAO,MAC9B,EAAD,CAAU,EAAG,EAAG,IAAK,KAClB,GAAD,CAAS,QAAc,mBAG3B,MAMC,GAGR,CAAC,CAAE,QAAO,kBAAmB,MAC1B,GAAW,EAAa,oBAAoB,GAC7B,SAAS,IAAK,OAAO,KAAK,EAAS,GAAG,MAAM,SAG9D,MAAD,OACG,EAAD,CAAS,KAAK,SAAQ,UAAQ,EAAS,UACtC,EAAD,CAAO,QAAQ,YAUZ,EAAD,KACG,EAAS,IAAI,CAAC,EAAS,MACrB,EAAD,CAAI,IAAK,KACN,EAAD,KAAK,EAAQ,OACZ,EAAD,OACG,GAAD,CAAiB,mBC5EpB,GAAuC,IAAM,MAClD,GAAS,GACb,CAAC,SAAU,UAAW,WACtB,CAAC,UAAW,WAAY,aAEpB,CAAC,EAAO,GAAY,GAAgB,MAAO,GAC3C,EAAK,EAAO,YAGf,GAAD,CAAY,QAAS,EAAG,QAAS,MAC9B,EAAD,OACG,EAAD,CAAS,KAAK,QACX,GAAD,CAAM,KAAK,gCAAgC,WAAU,GAAC,GAAG,OACtD,GAAD,MAAoB,kBACf,gDAGR,GAAD,QACC,GAAD,QACC,GAAD,SAED,EAAD,OACG,GAAD,SAED,GAAD,OACG,EAAD,CAAM,OAAQ,GAAM,SAAU,GAAM,SAAU,EAAU,QACrD,EAAD,OACG,EAAD,KAAK,wBACJ,EAAD,KAAK,WACJ,EAAD,KAAK,cAEN,EAAD,CAAW,EAAE,UACV,EAAD,OACG,GAAD,SAED,EAAD,OACG,GAAD,SAED,EAAD,OACG,GAAD,YC5DD,GAA4B,MAEpC,MAAD,OACG,GAAD,OCCO,GAA2B,IAAM,MAGtC,GAAkB,IAClB,EAAqB,EAAS,AAAC,GAAU,EAAM,oBAC/C,EAAqB,EAAS,AAAC,GAAU,EAAM,6BAC3C,IAAM,IACV,CAAC,EAAgB,sBAGf,GAA0B,YACtB,KAAK,EAAgB,UAAU,QAAQ,WAAY,AAAC,GAA+B,CACvF,KACiB,QAIb,KAAK,EAAgB,UAAU,QAAQ,aAAc,AAAC,GAA6C,CACvG,KACiB,MAIhB,IAAM,MACL,EAAU,OAAS,KAAa,UAEvC,CAAC,EAAgB,UAAW,MAEvB,GAAD,OC7BT,EAAY,OAAO,CACjB,OAAQ,EAAY,UACpB,KAAM,sBACN,QAAS,EACT,UAAW,SACX,YAAa,qCAGfgB,KACG,GAAD,OACG,GAAD,OACG,GAAD,QAGJ,SAAS,eAAe"}